---
description: Automatiza la creaci√≥n del notebook `01_data_discovery.ipynb` para la extracci√≥n incremental de datos desde Supabase hacia `data/01_raw/` en formato Parquet. Toda la l√≥gica de extracci√≥n se implementa directamente en el notebook (Fase Lab).
---

# Workflow: Fase 1 - Creaci√≥n del Notebook de Discovery (Data Extraction)

Este flujo de trabajo tiene como objetivo generar autom√°ticamente el notebook `notebooks/01_data_discovery.ipynb`. En esta fase de exploraci√≥n (Lab), la l√≥gica de carga incremental se implementa directamente dentro del notebook, utilizando √∫nicamente `src/connectors/supabase_connector.py` como dependencia externa.

## üõ†Ô∏è Pasos de Ejecuci√≥n

### Paso 1: Validaci√≥n de Preparativos (Pre-flight Check)
* **Acci√≥n:** Verificar que `config.yaml` contenga:
    * `data.source_tables`: Lista de tablas a gestionar.
    * `data.full_update`: Flag de carga total.
    * `data.date_column`: Columna pivote para incrementales.
* **Acci√≥n:** Verificar que `.env` tenga credenciales v√°lidas.

### Paso 2: Generaci√≥n del Notebook (`notebooks/01_data_discovery.ipynb`)
El notebook debe ser autocontenido y estructurado celda por celda:

* **Celda 1: Setup:**
    * Imports (`pandas`, `yaml`, `pathlib`).
    * Importar conector: `from src.connectors.supabase_connector import get_supabase_client`.
    * Cargar configuraci√≥n desde `config.yaml`.
    * Definir rutas (`RAW_DATA_PATH`).

* **Celda 2: Definici√≥n de Funciones de Carga (In-Notebook):**
    * Funci√≥n `get_remote_max_date(table)`: Consulta `select max(fecha)` a Supabase.
    * Funci√≥n `download_data(table, greater_than=None)`: Descarga datos (total o incremental).
    * Funci√≥n `sync_table(table_name, full_update)`:
        * Verifica si existe `{table}.parquet` local.
        * Si no existe o `full_update=True` -> `download_data(all)`.
        * Si existe -> Compara `max_local` vs `max_remote`.
            * Iguales -> `Pass`.
            * Remoto > Local -> `download_data(> max_local)` y `append`.

* **Celda 3: Ejecuci√≥n del Pipeline:**
    * Iterar sobre `config['data']['source_tables']`.
    * Ejecutar `sync_table()` para cada una.
    * Imprimir logs claros: "Tabla X: [STATUS] (Registros: N)".

* **Celda 4: Validaci√≥n de Salud (Sanity Check) y Generaci√≥n de Reporte:**
    * Leer cada parquet generado en `data/01_raw/`.
    * Mostrar `.info()` y `.head()` de cada uno.
    * **Validaci√≥n Cr√≠tica:** Verificar si la tabla de ventas tiene al menos 36 meses de historia (seg√∫n la regla de negocio).
    * **Generaci√≥n de Reporte (JSON):**
        * Estructurar el JSON para incluir la secci√≥n `download_details` *antes* de `validation_details`.
        * Para cada tabla, registrar:
            * `status`: "Full Download", "Incremental Download" o "Up to Date".
            * `new_rows`: N√∫mero de filas descargadas (0 si est√° al d√≠a).
            * `total_rows`: Total de filas en el archivo local despues de la operaci√≥n.
            * `download_timestamp`: Fecha y hora de la operaci√≥n.

### Paso 3: Ejecuci√≥n Manual y Verificaci√≥n
* **Acci√≥n:** El usuario abrir√° y ejecutar√° manualmente el notebook `notebooks/01_data_discovery.ipynb`.

### Paso 4: Limpieza de Archivos Temporales
* **Acci√≥n:** Eliminar cualquier archivo temporal generado durante la ejecuci√≥n (`.py`, `.txt`, `.log`) que no forme parte del repositorio.
* **Comando Sugerido:** `Remove-Item -Path "notebooks/run_*.py", "notebooks/*.log", "notebooks/*.txt" -ErrorAction SilentlyContinue`
* **Motivo:** Mantener el entorno de notebooks limpio y evitar commits de scripts de ejecuci√≥n temporal.
