---
description: Automatiza la creaci√≥n del notebook `01_data_discovery.ipynb` para la extracci√≥n incremental de datos desde Supabase hacia `data/01_raw/` en formato Parquet. Toda la l√≥gica de extracci√≥n se implementa directamente en el notebook (Fase Lab).
---

# Workflow: Fase 1 - Creaci√≥n del Notebook de Discovery (Data Extraction)

Este flujo de trabajo tiene como objetivo generar autom√°ticamente el notebook `notebooks/01_data_discovery.ipynb`. En esta fase de exploraci√≥n (Lab), la l√≥gica de carga incremental se implementa directamente dentro del notebook, utilizando √∫nicamente `src/connectors/supabase_connector.py` como dependencia externa.

## üõ†Ô∏è Pasos de Ejecuci√≥n

### Paso 1: Validaci√≥n de Preparativos (Pre-flight Check)
* **Acci√≥n:** Verificar que `config.yaml` contenga:
    * `data.source_tables`: Lista de tablas a gestionar.
    * `data.full_update`: Flag de carga total.
    * `data.date_column`: Columna pivote para incrementales.
* **Acci√≥n:** Verificar que `.env` tenga credenciales v√°lidas.

### Paso 2: Generaci√≥n del Notebook (`notebooks/01_data_discovery.ipynb`)
El notebook debe ser autocontenido y estructurado celda por celda:

* **Celda 1: Setup:**
    * Imports (`pandas`, `yaml`, `pathlib`).
    * Importar conector: `from src.connectors.supabase_connector import get_supabase_client`.
    * Cargar configuraci√≥n desde `config.yaml`.
    * Definir rutas (`RAW_DATA_PATH`).

* **Celda 2: Definici√≥n de Funciones de Carga (In-Notebook):**
    * Funci√≥n `get_remote_max_date(table)`: Consulta `select max(fecha)` a Supabase.
    * Funci√≥n `download_data(table, greater_than=None)`: Descarga datos (total o incremental).
    * Funci√≥n `sync_table(table_name, full_update)`:
        * Verifica si existe `{table}.parquet` local.
        * Si no existe o `full_update=True` -> `download_data(all)`.
        * Si existe -> Compara `max_local` vs `max_remote`.
            * Iguales -> `Pass`.
            * Remoto > Local -> `download_data(> max_local)` y `append`.

* **Celda 3: Ejecuci√≥n del Pipeline:**
    * Iterar sobre `config['data']['source_tables']`.
    * Ejecutar `sync_table()` para cada una.
    * Imprimir logs claros: "Tabla X: [STATUS] (Registros: N)".

* **Celda 4: Validaci√≥n de Salud (Sanity Check):**
    * Leer cada parquet generado en `data/01_raw/`.
    * Mostrar `.info()` y `.head()` de cada uno.
    * **Validaci√≥n Cr√≠tica:** Verificar si la tabla de ventas tiene al menos 36 meses de historia (seg√∫n la regla de negocio).

* **Celda 5: An√°lisis Estad√≠stico (Variables Num√©ricas):**
    * Para cada variable num√©rica detectada:
        * Calcular: Media, Mediana, Desviaci√≥n Est√°ndar.
        * Calcular: M√≠nimo, M√°ximo.
        * Calcular: Percentiles (25, 50, 75).
    * Almacenar resultados en diccionario de metadatos.

* **Celda 6: An√°lisis Temporal (Variables Datetime):**
    * Para cada variable datetime detectada:
        * Identificar fecha m√≠nima y m√°xima.
        * Detectar fechas faltantes (gaps) si aplica (especialmente en series de tiempo diarias).
        * Detectar fechas duplicadas.
    * Almacenar resultados.

* **Celda 7: An√°lisis Categ√≥rico (Variables Object):**
    * Para cada variable categ√≥rica (object):
        * Identificar valores √∫nicos.
        * Calcular frecuencia y peso relativo (%) de cada categor√≠a.
    * Almacenar resultados.

* **Celda 8: Detecci√≥n de Valores At√≠picos (Outliers):**
    * Para variables num√©ricas (int, float):
        * Definir l√≠mites (ej. IQR o Z-score).
        * Contar at√≠picos superiores e inferiores.
        * Identificar l√≠mites de corte.
    * Almacenar resultados.

* **Celda 9: Detecci√≥n de Varianza Cero (Zero Variance):**
    * Para todas las columnas de todos los archivos:
        * Verificar si la columna tiene un √∫nico valor (varianza cero).
        * Registrar las columnas que no aportan informaci√≥n.

* **Celda 10: Detecci√≥n de Alta Cardinalidad:**
    * Leer par√°metro `high_cardinality_threshold` desde `config.yaml` (secci√≥n `quality`).
    * Para todas las columnas (especialmente categ√≥ricas/object y discretas):
        * Calcular cardinalidad (valores √∫nicos).
        * Si cardinalidad > threshold, marcar como Alta Cardinalidad.
        * (Opcional) Calcular ratio de cardinalidad si el threshold es relativo (0-1).

* **Celda 11: Detecci√≥n de Presencia de Ceros:**
    * Leer par√°metro `zero_presence_threshold` desde `config.yaml` (opcional, o usar default).
    * Para todas las variables num√©ricas:
        * Calcular porcentaje de valores que son exactamente 0.
        * Si supera el umbral, reportar alta presencia de ceros.

* **Celda 12: Detecci√≥n de Filas Repetidas:**
    * Para cada archivo/tabla:
        * Identificar n√∫mero de filas duplicadas (exact matches).
        * Reportar si existen duplicados, indicando la cantidad.

* **Celda 13: Detecci√≥n de Valores Nulos:**
    * Para todas las columnas de todos los archivos:
        * Calcular cantidad y porcentaje de valores `NaN` o `None`.
        * Identificar columnas con alta presencia de nulos.
        * Reportar columnas afectadas y magnitud del problema.

* **Celda 14: Informe de Valores Centinela (Sentinel Values):**
    * Leer diccionario `sentinel_values` desde `config.yaml` (por tipo: `numeric`, `categorical`, `datetime`, `boolean`).
    * Para cada columna del archivo:
        * Determinar tipo de dato.
        * Buscar coincidencias exactas con los valores centinela configurados para ese tipo.
        * Reportar: `{ "columna": "nombre", "valor_centinela": valor, "conteo": n }`.

* **Celda 15: Validaci√≥n de Contrato de Datos (Data Contract):**
    * Leer secci√≥n `data_contract` desde `config.yaml`.
    * Para cada archivo/tabla de datos fuente (`ventas_diarias`, `macro_economia`, `promocion_diaria`, `redes_sociales`):
        * Validar que todas las columnas esperadas est√©n presentes.
        * Validar que el tipo de datos coincida con lo esperado (`int`, `float`, `datetime`, `object`).
        * Detectar y reportar columnas adicionales (extra columns) no definidas en el contrato.
        * Generar estado `PASS` o `FAIL` para cada tabla.

* **Celda 16: Validaci√≥n de Salud Financiera (Business Rules):**
    * Iterar `target_files` desde `config['financial_health']`.
    * Aplicar Regla 2.1: `total_unidades_entregadas` == SUM(`unidades_precio_normal`, `unidades_promo_pagadas`, `unidades_promo_bonificadas`).
    * Aplicar Regla 2.2: `unidades_promo_pagadas` == `unidades_promo_bonificadas`.
    * Aplicar Regla 2.3: `precio_unitario_full` >= `costo_unitario`.
    * Aplicar Regla 2.4: `utilidad` == `ingresos_totales` - `costo_total`.
    * Aplicar Regla 2.5: `ingresos_totales` == (`unidades_precio_normal` + `unidades_promo_pagadas`) * `precio_unitario_full`.
    * Aplicar Regla 2.6: `costo_total` == `total_unidades_entregadas` * `costo_unitario`.
    * Aplicar Regla 2.7: NO valores negativos en columnas num√©ricas.
    * Almacenar resultados de validaci√≥n en `TABLE_ANALYSIS[table]['financial_health']`.

* **Celda 17: Generaci√≥n de Reporte JSON:**
    * Recopilar todos los diccionarios de resultados:
        * `download_details`
        * `TABLE_ANALYSIS` (estad√≠sticas, validaciones temporales, categoricas, outliers, varianza cero, cardinalidad, presencia de ceros, duplicados, nulos, centinelas, contrato de datos, salud financiera).
    * Estructurar el JSON final:
        ```json
        {
          "phase": "Phase 1 - Data Discovery",
          "timestamp": "ISO 8601",
          "description": "...",
          "download_details": [...],
          "data_analysis": { ... }
        }
        ```
    * Guardar en `experiments/phase_01_discovery/artifacts/phase_01_discovery.json`.
    * Imprimir mensaje de confirmaci√≥n.

### Paso 3: Ejecuci√≥n Manual y Verificaci√≥n
* **Acci√≥n:** El usuario abrir√° y ejecutar√° manualmente el notebook `notebooks/01_data_discovery.ipynb`.

### Paso 4: Limpieza de Archivos Temporales
* **Acci√≥n:** Eliminar cualquier archivo temporal generado durante la ejecuci√≥n (`.py`, `.txt`, `.log`) que no forme parte del repositorio.
* **Comando Sugerido:** `Remove-Item -Path "notebooks/run_*.py", "notebooks/*.log", "notebooks/*.txt" -ErrorAction SilentlyContinue`
* **Motivo:** Mantener el entorno de notebooks limpio y evitar commits de scripts de ejecuci√≥n temporal.
