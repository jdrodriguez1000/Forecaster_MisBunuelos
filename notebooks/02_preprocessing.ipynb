{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fase 2: Robust Data Preprocessing\n",
                "\n",
                "Este notebook implementa el pipeline de limpieza, validaci\u00f3n, imputaci\u00f3n y agregaci\u00f3n mensual de los datos crudos para el proyecto **Forecaster Mis Bu\u00f1uelos**.\n",
                "\n",
                "**Objetivo:** Generar `data/02_cleansed/master_monthly.parquet`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 1: Setup\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import yaml\n",
                "from pathlib import Path\n",
                "import os\n",
                "from datetime import datetime\n",
                "import platform\n",
                "\n",
                "# Configurar pandas\n",
                "pd.set_option('display.max_columns', None)\n",
                "\n",
                "# Definir Rutas\n",
                "BASE_DIR = Path(os.getcwd())\n",
                "if BASE_DIR.name == \"notebooks\":\n",
                "    BASE_DIR = BASE_DIR.parent\n",
                "\n",
                "CONFIG_PATH = BASE_DIR / \"config.yaml\"\n",
                "RAW_DATA_PATH = BASE_DIR / \"data\" / \"01_raw\"\n",
                "CLEANSED_DATA_PATH = BASE_DIR / \"data\" / \"02_cleansed\"\n",
                "ARTIFACTS_PATH = BASE_DIR / \"experiments\" / \"phase_02_preprocessing\" / \"artifacts\"\n",
                "\n",
                "# Crear directorios\n",
                "CLEANSED_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
                "ARTIFACTS_PATH.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Cargar Configuraci\u00f3n\n",
                "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
                "    config = yaml.safe_load(f)\n",
                "\n",
                "print(\"Configuraci\u00f3n cargada y rutas establecidas.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 2: Carga de Datos Crudos\n",
                "files = {\n",
                "    \"ventas\": RAW_DATA_PATH / \"ventas_diarias.parquet\",\n",
                "    \"marketing\": RAW_DATA_PATH / \"redes_sociales.parquet\",\n",
                "    \"promo\": RAW_DATA_PATH / \"promocion_diaria.parquet\",\n",
                "    \"macro\": RAW_DATA_PATH / \"macro_economia.parquet\"\n",
                "}\n",
                "\n",
                "dataframes = {}\n",
                "print(\"Cargando archivos:\")\n",
                "for key, path in files.items():\n",
                "    if path.exists():\n",
                "        df = pd.read_parquet(path)\n",
                "        dataframes[key] = df\n",
                "        print(f\"  - {key}: {df.shape}\")\n",
                "    else:\n",
                "        raise FileNotFoundError(f\"Archivo no encontrado: {path}\")\n",
                "\n",
                "# Referencias directas para facilitar el c\u00f3digo subsiguiente\n",
                "df_ventas = dataframes[\"ventas\"]\n",
                "df_marketing = dataframes[\"marketing\"]\n",
                "df_promo = dataframes[\"promo\"]\n",
                "df_macro = dataframes[\"macro\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 3: Validaci\u00f3n de Contrato de Datos (Critical Check)\n",
                "print(\"Validando Contratos de Datos...\")\n",
                "data_contract = config.get(\"data_contract\", {})\n",
                "\n",
                "# Mapa de keys internas a keys del config\n",
                "config_map = {\n",
                "    \"ventas\": \"ventas_diarias\",\n",
                "    \"marketing\": \"redes_sociales\",\n",
                "    \"promo\": \"promocion_diaria\",\n",
                "    \"macro\": \"macro_economia\"\n",
                "}\n",
                "\n",
                "data_contract_status = {}\n",
                "    \n",
                "for key, df in dataframes.items():\n",
                "    config_name = config_map.get(key)\n",
                "    # Obtener columnas esperadas\n",
                "    expected_cols = list(data_contract.get(config_name, {}).keys())\n",
                "    \n",
                "    missing_cols = [col for col in expected_cols if col not in df.columns]\n",
                "    \n",
                "    if missing_cols:\n",
                "        error_msg = f\"ERROR CR\u00cdTICO en {key}: Faltan columnas {missing_cols}\"\n",
                "        print(error_msg)\n",
                "        data_contract_status[key] = f\"FAILED: Missing {missing_cols}\"\n",
                "        raise RuntimeError(error_msg)\n",
                "    else:\n",
                "        print(f\"  - {key}: Validaci\u00f3n de Contrato OK\")\n",
                "        data_contract_status[key] = \"OK\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 4: Estandarizaci\u00f3n de Nombres (Cleaning)\n",
                "rename_map = config[\"preprocessing\"].get(\"rename_map\") or {}\n",
                "print(f\"Aplicando rename_map: {rename_map}\")\n",
                "\n",
                "for key, df in dataframes.items():\n",
                "    # Renombrar columnas\n",
                "    df.rename(columns=rename_map, inplace=True)\n",
                "    # Convertir a snake_case (opcional pero recomendado)\n",
                "    df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]\n",
                "    dataframes[key] = df\n",
                "\n",
                "print(\"Nombres estandarizados.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 5: Selecci\u00f3n de Columnas (Schema Enforcement)\n",
                "print(\"Aplicando Selecci\u00f3n de Columnas (Schema Enforcement)...\")\n",
                "columns_removed_log = {}\n",
                "\n",
                "for key, df in dataframes.items():\n",
                "    config_name = config_map.get(key)\n",
                "    # Obtener columnas esperadas originales del contrato\n",
                "    original_expected_cols = list(data_contract.get(config_name, {}).keys())\n",
                "    \n",
                "    # Aplicar el mismo mapeo de nombres a la lista esperada\n",
                "    final_expected_cols = []\n",
                "    for col in original_expected_cols:\n",
                "        # Si 'col' est\u00e1 en rename_map keys, usa el value, si no usa 'col'\n",
                "        # Luego aplica lower() y replace() igual que hicimos con el df\n",
                "        new_name = rename_map.get(col, col).lower().replace(\" \", \"_\")\n",
                "        final_expected_cols.append(new_name)\n",
                "    \n",
                "    # Filtrar el DataFrame\n",
                "    cols_to_keep = [col for col in df.columns if col in final_expected_cols]\n",
                "    removed = [col for col in df.columns if col not in final_expected_cols]\n",
                "    \n",
                "    if removed:\n",
                "        columns_removed_log[key] = removed\n",
                "    \n",
                "    dataframes[key] = df[cols_to_keep].copy()\n",
                "\n",
                "print(\"Columnas eliminadas por no estar en contrato:\", columns_removed_log)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 6: Limpieza de Filas (Duplicados y Ruido)\n",
                "print(\"Limpieza de Filas...\")\n",
                "stats_cleaning = {\"duplicates\": {}, \"filtered\": {}}\n",
                "filters = config[\"preprocessing\"].get(\"filters\", {})\n",
                "min_date = pd.to_datetime(filters.get(\"min_date\", \"2018-01-01\"))\n",
                "\n",
                "for key, df in dataframes.items():\n",
                "    initial_rows = len(df)\n",
                "    \n",
                "    # 1. Deduplicaci\u00f3n Exacta\n",
                "    df = df.drop_duplicates()\n",
                "    \n",
                "    # 2. Deduplicaci\u00f3n Temporal (Keep Last)\n",
                "    if \"fecha\" in df.columns:\n",
                "        df[\"fecha\"] = pd.to_datetime(df[\"fecha\"])\n",
                "        df = df.sort_values(\"fecha\")\n",
                "        # Eliminar duplicados de fecha conservando el \u00faltimo\n",
                "        duplicates_date = df.duplicated(subset=[\"fecha\"], keep=\"last\")\n",
                "        df = df[~duplicates_date]\n",
                "    \n",
                "    rows_after_dedup = len(df)\n",
                "    stats_cleaning[\"duplicates\"][key] = initial_rows - rows_after_dedup\n",
                "    \n",
                "    # 3. Filtrado por Fecha\n",
                "    if \"fecha\" in df.columns:\n",
                "        df = df[df[\"fecha\"] >= min_date]\n",
                "        stats_cleaning[\"filtered\"][key] = rows_after_dedup - len(df)\n",
                "    \n",
                "    dataframes[key] = df\n",
                "\n",
                "print(\"Estad\u00edsticas de Limpieza:\", stats_cleaning)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 7: Tratamiento de Valores Centinela\n",
                "print(\"Tratamiento de Centinelas...\")\n",
                "sentinel_values = config[\"quality\"].get(\"sentinel_values\", {})\n",
                "numeric_sentinels = sentinel_values.get(\"numeric\", []) # ej [-1, 999]\n",
                "text_sentinels = sentinel_values.get(\"text\", [])\n",
                "\n",
                "sentinel_stats = {}\n",
                "\n",
                "for key, df in dataframes.items():\n",
                "    count_replaced = 0\n",
                "    for col in df.columns:\n",
                "        # Excepci\u00f3n confianza_consumidor\n",
                "        is_confianza = (key == \"macro\" and col == \"confianza_consumidor\")\n",
                "        \n",
                "        if pd.api.types.is_numeric_dtype(df[col]):\n",
                "            for val in numeric_sentinels:\n",
                "                if is_confianza and val == -1:\n",
                "                    continue # No reemplazar -1 en confianza\n",
                "                \n",
                "                mask = (df[col] == val)\n",
                "                if mask.any():\n",
                "                    count_replaced += mask.sum()\n",
                "                    df.loc[mask, col] = np.nan\n",
                "                    \n",
                "        elif pd.api.types.is_string_dtype(df[col]):\n",
                "             for val in text_sentinels:\n",
                "                mask = (df[col] == val)\n",
                "                if mask.any():\n",
                "                    count_replaced += mask.sum()\n",
                "                    df.loc[mask, col] = np.nan\n",
                "    \n",
                "    sentinel_stats[key] = int(count_replaced)\n",
                "    dataframes[key] = df\n",
                "\n",
                "print(\"Centinelas reemplazados por NaN:\", sentinel_stats)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 8: Garantizar Completitud Temporal (Reindexing)\n",
                "print(\"Garantizar Completitud Temporal...\")\n",
                "# Determinar rango completo global (desde min_date hasta maximo encontrado)\n",
                "all_max_dates = [df[\"fecha\"].max() for df in dataframes.values() if \"fecha\" in df.columns]\n",
                "global_max_date = max(all_max_dates) if all_max_dates else datetime.now()\n",
                "\n",
                "# Obtener frecuencias configuradas por fuente\n",
                "freq_map = config[\"preprocessing\"].get(\"data_frequency\", {})\n",
                "# Mapeo interno: key_notebook -> key_config\n",
                "key_map = {\n",
                "    \"ventas\": \"ventas_diarias\",\n",
                "    \"marketing\": \"redes_sociales\",\n",
                "    \"promo\": \"promocion_diaria\",\n",
                "    \"macro\": \"macro_economia\"\n",
                "}\n",
                "\n",
                "reindex_stats = {}\n",
                "\n",
                "for key, df in dataframes.items():\n",
                "    if \"fecha\" in df.columns:\n",
                "        config_key = key_map.get(key)\n",
                "        freq = freq_map.get(config_key, \"D\") # Default a Diario si no est\u00e1 config\n",
                "        \n",
                "        print(f\"  - Reindexando {key} con frecuencia: {freq}\")\n",
                "        \n",
                "        # Crear \u00edndice espec\u00edfico para esta fuente con su frecuencia correcta\n",
                "        full_idx = pd.date_range(start=min_date, end=global_max_date, freq=freq, name=\"fecha\")\n",
                "        \n",
                "        # Set index fecha\n",
                "        df = df.set_index(\"fecha\")\n",
                "        # Eliminar duplicados de \u00edndice si quedaran (por seguridad)\n",
                "        df = df[~df.index.duplicated(keep='last')]\n",
                "        \n",
                "        # Reindexar\n",
                "        original_len = len(df)\n",
                "        df = df.reindex(full_idx)\n",
                "        df.index.name = \"fecha\" # Restaurar nombre\n",
                "        \n",
                "        # Reset index para volver a tener columna fecha\n",
                "        df = df.reset_index()\n",
                "        \n",
                "        new_len = len(df)\n",
                "        reindex_stats[key] = new_len - original_len\n",
                "        dataframes[key] = df\n",
                "\n",
                "print(\"Filas a\u00f1adidas (huecos temporales recuperados) por reindexado:\", reindex_stats)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 9: Imputaci\u00f3n de Nulos (L\u00f3gica de Negocio)\n",
                "print(\"Ejecutando Imputaci\u00f3n de Negocio...\")\n",
                "imputation_stats = {\n",
                "    \"macro\": {},\n",
                "    \"promo\": {},\n",
                "    \"marketing\": {},\n",
                "    \"ventas\": {}\n",
                "}\n",
                "\n",
                "# Actualizar referencias locales tras reindexado\n",
                "df_ventas = dataframes[\"ventas\"]\n",
                "df_marketing = dataframes[\"marketing\"]\n",
                "df_promo = dataframes[\"promo\"]\n",
                "df_macro = dataframes[\"macro\"]\n",
                "\n",
                "# --- Macroeconom\u00eda ---\n",
                "# Media m\u00f3vil 2 meses anteriores\n",
                "cols_num_macro = df_macro.select_dtypes(include=np.number).columns\n",
                "for col in cols_num_macro:\n",
                "    nulls_before = df_macro[col].isna().sum()\n",
                "    if nulls_before > 0:\n",
                "        # Rolling mean shift 1 para causalidad\n",
                "        df_macro[col] = df_macro[col].fillna(\n",
                "            df_macro[col].rolling(window=60, min_periods=1).mean().shift(1)\n",
                "        )\n",
                "        # Fallback para inicio de serie\n",
                "        df_macro[col] = df_macro[col].fillna(method='bfill')\n",
                "        imputation_stats[\"macro\"][col] = int(nulls_before)\n",
                "\n",
                "# --- Promociones ---\n",
                "if \"es_promo\" in df_promo.columns:\n",
                "    mask_null_promo = df_promo[\"es_promo\"].isna()\n",
                "    count_promo_nulls = mask_null_promo.sum()\n",
                "    if count_promo_nulls > 0:\n",
                "        meses_promo = [4, 5, 9, 10]\n",
                "        months = df_promo[\"fecha\"].dt.month\n",
                "        # Meses promo -> 1, Resto -> 0\n",
                "        df_promo.loc[mask_null_promo & months.isin(meses_promo), \"es_promo\"] = 1\n",
                "        df_promo.loc[mask_null_promo & ~months.isin(meses_promo), \"es_promo\"] = 0\n",
                "        imputation_stats[\"promo\"][\"es_promo_inferred\"] = int(count_promo_nulls)\n",
                "\n",
                "# --- Marketing (Redes) ---\n",
                "# 1. Campa\u00f1a\n",
                "target_col_campana = \"ciclo\" if \"ciclo\" in df_marketing.columns else \"campana\"\n",
                "mask_camp_null = df_marketing[target_col_campana].isna()\n",
                "count_campana_nulls = mask_camp_null.sum()\n",
                "    \n",
                "if count_campana_nulls > 0:\n",
                "    fb_val = df_marketing[\"inversion_facebook\"].fillna(0)\n",
                "    ig_val = df_marketing[\"inversion_instagram\"].fillna(0)\n",
                "    has_inv = (fb_val > 0) | (ig_val > 0)\n",
                "\n",
                "    months = df_marketing[\"fecha\"].dt.month\n",
                "    mask_abr_may = months.isin([3, 4, 5])\n",
                "    mask_sep_oct = months.isin([8, 9, 10])\n",
                "\n",
                "    df_marketing.loc[mask_camp_null & has_inv & mask_abr_may, target_col_campana] = \"Ciclo Abr-May\"\n",
                "    df_marketing.loc[mask_camp_null & has_inv & mask_sep_oct, target_col_campana] = \"Ciclo Sep-Oct\"\n",
                "    df_marketing.loc[mask_camp_null & df_marketing[target_col_campana].isna(), target_col_campana] = \"Sin Campa\u00f1a\"\n",
                "    imputation_stats[\"marketing\"][\"campaigns_inferred\"] = int(count_campana_nulls)\n",
                "\n",
                "# 2. Inversiones\n",
                "fechas = df_marketing[\"fecha\"]\n",
                "rango1 = (((fechas.dt.month == 3) & (fechas.dt.day >= 15)) | (fechas.dt.month == 4) | ((fechas.dt.month == 5) & (fechas.dt.day <= 25)))\n",
                "rango2 = (((fechas.dt.month == 8) & (fechas.dt.day >= 15)) | (fechas.dt.month == 9) | ((fechas.dt.month == 10) & (fechas.dt.day <= 25)))\n",
                "rango_activo = rango1 | rango2\n",
                "\n",
                "for col in [\"inversion_facebook\", \"inversion_instagram\"]:\n",
                "    if col in df_marketing.columns:\n",
                "        mask_null = df_marketing[col].isna()\n",
                "        count_inv_nulls = mask_null.sum()\n",
                "        if count_inv_nulls > 0:\n",
                "            mask_null_in_range = mask_null & rango_activo\n",
                "            if mask_null_in_range.any():\n",
                "                df_marketing[col] = df_marketing[col].interpolate(method='linear')\n",
                "            \n",
                "            mask_null_out_range = mask_null & ~rango_activo\n",
                "            if mask_null_out_range.any():\n",
                "                df_marketing.loc[mask_null_out_range, col] = 0\n",
                "            \n",
                "            imputation_stats[\"marketing\"][f\"{col}_imputed\"] = int(count_inv_nulls)\n",
                "\n",
                "# 3. Consistencia Total\n",
                "target_col_marketing = \"inversion_marketing_total\" if \"inversion_marketing_total\" in df_marketing.columns else \"inversion_total_diaria\"\n",
                "if target_col_marketing in df_marketing.columns:\n",
                "    df_marketing[target_col_marketing] = df_marketing[\"inversion_facebook\"] + df_marketing[\"inversion_instagram\"]\n",
                "\n",
                "# --- Ventas Diarias ---\n",
                "# Identificar filas IMPUTADAS (Total era Null originalmente despu\u00e9s del reindexado)\n",
                "imputed_sales_mask = df_ventas[\"total_unidades_entregadas\"].isna()\n",
                "imputation_stats[\"ventas\"][\"dates_missing_imputed\"] = int(imputed_sales_mask.sum())\n",
                "\n",
                "# Precios/Costos -> ffill, bfill\n",
                "for col in [\"precio_unitario_full\", \"costo_unitario\"]:\n",
                "    if col in df_ventas.columns:\n",
                "        nulls = df_ventas[col].isna().sum()\n",
                "        if nulls > 0:\n",
                "            df_ventas[col] = df_ventas[col].ffill().bfill()\n",
                "            imputation_stats[\"ventas\"][f\"{col}_filled\"] = int(nulls)\n",
                "\n",
                "# Unidades Total\n",
                "if \"total_unidades_entregadas\" in df_ventas.columns:\n",
                "    s_total = df_ventas[\"total_unidades_entregadas\"]\n",
                "    # Interpolaci\u00f3n Lineal (cubre gaps peque\u00f1os y \"unir puntos\")\n",
                "    s_interp = s_total.interpolate(method='linear')\n",
                "    df_ventas[\"total_unidades_entregadas\"] = s_interp.fillna(0)\n",
                "\n",
                "# Desglose\n",
                "for col in [\"unidades_promo_pagadas\", \"unidades_promo_bonificadas\"]:\n",
                "    if col in df_ventas.columns:\n",
                "        df_ventas[col] = df_ventas[col].fillna(0)\n",
                "\n",
                "if \"unidades_precio_normal\" in df_ventas.columns:\n",
                "    # Residual\n",
                "    residual = df_ventas[\"total_unidades_entregadas\"] - (df_ventas[\"unidades_promo_pagadas\"] + df_ventas[\"unidades_promo_bonificadas\"])\n",
                "    df_ventas[\"unidades_precio_normal\"] = df_ventas[\"unidades_precio_normal\"].fillna(residual)\n",
                "    # Clip a 0\n",
                "    df_ventas[\"unidades_precio_normal\"] = df_ventas[\"unidades_precio_normal\"].clip(lower=0)\n",
                "\n",
                "print(\"Imputaci\u00f3n de Negocio completada.\")\n",
                "print(\"Stats Imputaci\u00f3n:\", imputation_stats)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 10: Rec\u00e1lculo Financiero Selectivo\n",
                "print(\"Rec\u00e1lculo Financiero Selectivo...\")\n",
                "recalc_flag = config[\"preprocessing\"].get(\"recalc_financials\", False)\n",
                "\n",
                "if recalc_flag:\n",
                "    # Solo filas imputadas en ventas\n",
                "    if imputed_sales_mask.any():\n",
                "        count = imputed_sales_mask.sum()\n",
                "        print(f\"Recalculando {count} filas imputadas...\")\n",
                "        \n",
                "        # Indices\n",
                "        idx = df_ventas[imputed_sales_mask].index\n",
                "        \n",
                "        # Costo Total\n",
                "        df_ventas.loc[idx, \"costo_total\"] = (\n",
                "            df_ventas.loc[idx, \"total_unidades_entregadas\"] * \n",
                "            df_ventas.loc[idx, \"costo_unitario\"]\n",
                "        )\n",
                "        \n",
                "        # Ingresos Totales (Normal + Promo Pagada) * Precio\n",
                "        unidades_pagas = (\n",
                "            df_ventas.loc[idx, \"unidades_precio_normal\"] + \n",
                "            df_ventas.loc[idx, \"unidades_promo_pagadas\"]\n",
                "        )\n",
                "        # FIX: Usar precio_unitario_full\n",
                "        df_ventas.loc[idx, \"ingresos_totales\"] = (\n",
                "            unidades_pagas * df_ventas.loc[idx, \"precio_unitario_full\"]\n",
                "        )\n",
                "        \n",
                "        # Utilidad\n",
                "        df_ventas.loc[idx, \"utilidad\"] = (\n",
                "            df_ventas.loc[idx, \"ingresos_totales\"] - \n",
                "            df_ventas.loc[idx, \"costo_total\"]\n",
                "        )\n",
                "    else:\n",
                "        print(\"No hay filas imputadas para recalcular.\")\n",
                "else:\n",
                "    print(\"Rec\u00e1lculo financiero desactivado en config.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 11: Agregaci\u00f3n Mensual\n",
                "print(\"Agregaci\u00f3n Mensual (MS)...\")\n",
                "agg_rules = config[\"preprocessing\"].get(\"aggregation_rules\", {})\n",
                "\n",
                "monthly_dfs = {}\n",
                "\n",
                "for key, df in dataframes.items():\n",
                "    if \"fecha\" in df.columns:\n",
                "        df = df.set_index(\"fecha\")\n",
                "    \n",
                "    # Filtrar reglas para este DF\n",
                "    current_rules = {col: agg_rules[col] for col in df.columns if col in agg_rules}\n",
                "    \n",
                "    # Reglas especiales\n",
                "    if key == \"promo\" and \"es_promo\" in df.columns:\n",
                "        current_rules[\"es_promo\"] = \"sum\" # Cuenta d\u00edas\n",
                "    elif key == \"macro\":\n",
                "        current_rules = {col: \"first\" for col in df.columns}\n",
                "    \n",
                "    # Resample\n",
                "    if current_rules:\n",
                "        df_monthly = df.resample(\"MS\").agg(current_rules)\n",
                "    else:\n",
                "        df_monthly = df.resample(\"MS\").sum(numeric_only=True)\n",
                "    \n",
                "    # Renombres post-agregaci\u00f3n\n",
                "    if key == \"promo\" and \"es_promo\" in df_monthly.columns:\n",
                "        df_monthly.rename(columns={\"es_promo\": \"dias_en_promo\"}, inplace=True)\n",
                "        \n",
                "    monthly_dfs[key] = df_monthly\n",
                "\n",
                "print(\"Agregaci\u00f3n completada.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 12: Unificaci\u00f3n de Fuentes (Merging)\n",
                "print(\"Unificando Datasets...\")\n",
                "df_master = monthly_dfs[\"ventas\"].copy()\n",
                "\n",
                "for key in [\"marketing\", \"promo\", \"macro\"]:\n",
                "    other_df = monthly_dfs[key]\n",
                "    # Merge por \u00edndice (fechas mes)\n",
                "    df_master = df_master.merge(other_df, left_index=True, right_index=True, how=\"left\")\n",
                "\n",
                "print(f\"Dataset Maestro Mensual: {df_master.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 13: Imputaci\u00f3n Post-Merge\n",
                "print(\"Imputaci\u00f3n Final (Huecos Estructurales)...\")\n",
                "df_master = df_master.interpolate(method='linear').ffill().bfill()\n",
                "\n",
                "nulos = df_master.isna().sum().sum()\n",
                "if nulos > 0:\n",
                "    print(f\"ADVERTENCIA: Quedan {nulos} valores nulos.\")\n",
                "else:\n",
                "    print(\"Dataset limpio.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 14: Regla de Oro (Anti-Data Leakage)\n",
                "print(\"Aplicando Regla de Oro: Eliminaci\u00f3n del mes en curso (incompleto)...\")\n",
                "current_date = datetime.now()\n",
                "if not df_master.empty:\n",
                "    last_date = df_master.index.max()\n",
                "    # Verificar si el \u00faltimo mes del dataset coincide con el mes/a\u00f1o actual\n",
                "    if last_date.year == current_date.year and last_date.month == current_date.month:\n",
                "        print(f\"  - Detectado mes incompleto o en curso: {last_date.strftime('%Y-%m')}. Eliminando para evitar Data Leakage.\")\n",
                "        df_master = df_master.iloc[:-1]\n",
                "    else:\n",
                "        print(f\"  - El \u00faltimo mes ({last_date.strftime('%Y-%m')}) es anterior al actual. No se requiere corte.\")\n",
                "\n",
                "print(f\"Dataset Maestro Final (Meses Cerrados): {df_master.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 15: Exportaci\u00f3n\n",
                "output_file = CLEANSED_DATA_PATH / \"master_monthly.parquet\"\n",
                "df_master.to_parquet(output_file)\n",
                "print(f\"Guardado exitoso en: {output_file}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Celda 16: Reporte JSON Detallado (Enhanced + Integrity Checks)\n",
                "import json\n",
                "import platform\n",
                "\n",
                "# Recopilar m\u00e9tricas finales del artefacto de salida\n",
                "final_shape = df_master.shape\n",
                "\n",
                "# 1. Validaciones Temporales\n",
                "is_series_complete = False\n",
                "missing_expected_dates = []\n",
                "duplicate_dates_count = 0\n",
                "date_min = \"N/A\"\n",
                "date_max = \"N/A\"\n",
                "total_months = 0\n",
                "\n",
                "if isinstance(df_master.index, pd.DatetimeIndex):\n",
                "    if not df_master.empty:\n",
                "        date_min = df_master.index.min().isoformat()\n",
                "        date_max = df_master.index.max().isoformat()\n",
                "        total_months = len(df_master)\n",
                "        \n",
                "        # Chequear completitud (Freq MS)\n",
                "        expected_range = pd.date_range(start=df_master.index.min(), end=df_master.index.max(), freq='MS')\n",
                "        # Comparar sets para ignorar duplicados en index por ahora (se revisan abajo)\n",
                "        is_series_complete = len(expected_range) == len(set(df_master.index))\n",
                "        if not is_series_complete:\n",
                "             missing_expected_dates = [d.isoformat() for d in set(expected_range) - set(df_master.index)]\n",
                "        \n",
                "        # Chequear fechas duplicadas\n",
                "        duplicate_dates_count = int(df_master.index.duplicated().sum())\n",
                "\n",
                "# 2. Integridad de Datos\n",
                "duplicate_rows = int(df_master.duplicated().sum())\n",
                "total_nulls = int(df_master.isna().sum().sum())\n",
                "rows_with_nulls = int(df_master.isna().any(axis=1).sum())\n",
                "column_types = df_master.dtypes.astype(str).to_dict()\n",
                "columns_list = df_master.columns.tolist()\n",
                "\n",
                "file_size_bytes = output_file.stat().st_size if output_file.exists() else 0\n",
                "\n",
                "missing_values = total_nulls\n",
                "\n",
                "# 3. Muestras de Datos (Serialize dates/timestamps for JSON)\n",
                "def serialize_df(df_part):\n",
                "    # Convert timestamp index to string column for JSON\n",
                "    temp = df_part.copy()\n",
                "    if isinstance(temp.index, pd.DatetimeIndex):\n",
                "        temp = temp.reset_index()\n",
                "        temp['fecha'] = temp['fecha'].astype(str)\n",
                "    return temp.to_dict(orient='records')\n",
                "\n",
                "head_5 = serialize_df(df_master.head(5))\n",
                "tail_5 = serialize_df(df_master.tail(5))\n",
                "random_5 = serialize_df(df_master.sample(5, random_state=42))\n",
                "\n",
                "# Estructura enriquecida del reporte\n",
                "report = {\n",
                "    \"phase\": \"Phase 2 - Preprocessing\",\n",
                "    \"timestamp\": datetime.now().isoformat(),\n",
                "    \"environment_info\": {\n",
                "        \"platform\": platform.system(),\n",
                "        \"python_version\": platform.python_version(),\n",
                "        \"pandas_version\": pd.__version__\n",
                "    },\n",
                "    \"execution_context\": {\n",
                "        \"description\": \"Limpieza exhaustiva, imputaci\u00f3n de negocio, agregaci\u00f3n mensual y corte de mes en curso (Anti-Data Leakage).\",\n",
                "        \"validation_status\": \"SUCCESS\" if total_nulls == 0 and is_series_complete and duplicate_dates_count == 0 else \"WARNING\"\n",
                "    },\n",
                "    \"data_quality_audit\": {\n",
                "        \"contract_validation\": data_contract_status if 'data_contract_status' in locals() else {},\n",
                "        \"schema_enforcement\": {\n",
                "            \"columns_removed\": columns_removed_log if 'columns_removed_log' in locals() else {}\n",
                "        },\n",
                "        \"cleaning_stats\": {\n",
                "            \"rows_filtered_logic\": stats_cleaning.get(\"filtered\", {}),\n",
                "            \"duplicates_removed\": stats_cleaning.get(\"duplicates\", {}),\n",
                "            \"sentinel_values_replaced\": sentinel_stats,\n",
                "            \"temporal_gaps_reindexed\": reindex_stats\n",
                "        },\n",
                "        \"imputation_metrics\": {\n",
                "            \"financial_records_recalculated\": int(imputed_sales_mask.sum()) if 'imputed_sales_mask' in locals() else 0,\n",
                "            \"remaining_nulls_final\": total_nulls,\n",
                "            \"details\": imputation_stats if 'imputation_stats' in locals() else {}\n",
                "        }\n",
                "    },\n",
                "    \"output_artifact_details\": {\n",
                "        \"file_name\": output_file.name,\n",
                "        \"full_path\": str(output_file.absolute()),\n",
                "        \"file_size_bytes\": file_size_bytes,\n",
                "        \"shape\": {\n",
                "            \"rows\": final_shape[0],\n",
                "            \"columns\": final_shape[1]\n",
                "        },\n",
                "        \"temporal_coverage\": {\n",
                "            \"start_date\": date_min,\n",
                "            \"end_date\": date_max,\n",
                "            \"frequency\": \"MS (Month Start)\",\n",
                "            \"total_months\": total_months,\n",
                "            \"is_series_complete\": is_series_complete,\n",
                "            \"missing_expected_dates\": missing_expected_dates,\n",
                "            \"duplicate_dates_count\": duplicate_dates_count\n",
                "        },\n",
                "        \"data_integrity\": {\n",
                "            \"duplicate_rows\": duplicate_rows,\n",
                "            \"rows_with_nulls\": rows_with_nulls,\n",
                "            \"total_nulls\": total_nulls\n",
                "        },\n",
                "        \"schema\": {\n",
                "            \"columns\": columns_list,\n",
                "            \"dtypes\": column_types\n",
                "        }\n",
                "    },\n",
                "    \"sample_data\": {\n",
                "        \"head_5\": head_5,\n",
                "        \"tail_5\": tail_5,\n",
                "        \"random_5\": random_5\n",
                "    }\n",
                "}\n",
                "\n",
                "# Guardar reporte\n",
                "report_path = ARTIFACTS_PATH / \"phase_02_preprocessing.json\"\n",
                "with open(report_path, \"w\", encoding='utf-8') as f:\n",
                "    json.dump(report, f, indent=4)\n",
                "\n",
                "print(f\"Reporte DETALLADO generado en: {report_path}\")\n",
                "print(\"Resumen del Artefacto de Salida:\")\n",
                "print(json.dumps(report[\"output_artifact_details\"], indent=2))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}