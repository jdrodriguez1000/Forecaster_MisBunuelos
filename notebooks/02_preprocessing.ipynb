{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa36a934",
   "metadata": {},
   "source": [
    "# Fase 2: Robust Data Preprocessing\n",
    "\n",
    "Este notebook implementa el pipeline de limpieza, validación, imputación y agregación mensual de los datos crudos para el proyecto **Forecaster Mis Buñuelos**.\n",
    "\n",
    "**Objetivo:** Generar `data/02_cleansed/master_monthly.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5812df12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T22:03:41.295013Z",
     "iopub.status.busy": "2026-02-14T22:03:41.295013Z",
     "iopub.status.idle": "2026-02-14T22:03:41.655974Z",
     "shell.execute_reply": "2026-02-14T22:03:41.655974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración cargada y rutas establecidas.\n"
     ]
    }
   ],
   "source": [
    "# Celda 1: Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "import platform\n",
    "\n",
    "# Configurar pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Definir Rutas\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "if BASE_DIR.name == \"notebooks\":\n",
    "    BASE_DIR = BASE_DIR.parent\n",
    "\n",
    "CONFIG_PATH = BASE_DIR / \"config.yaml\"\n",
    "RAW_DATA_PATH = BASE_DIR / \"data\" / \"01_raw\"\n",
    "CLEANSED_DATA_PATH = BASE_DIR / \"data\" / \"02_cleansed\"\n",
    "ARTIFACTS_PATH = BASE_DIR / \"experiments\" / \"phase_02_preprocessing\" / \"artifacts\"\n",
    "\n",
    "# Crear directorios\n",
    "CLEANSED_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cargar Configuración\n",
    "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuración cargada y rutas establecidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93050b2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T22:03:41.657988Z",
     "iopub.status.busy": "2026-02-14T22:03:41.657988Z",
     "iopub.status.idle": "2026-02-14T22:03:41.693865Z",
     "shell.execute_reply": "2026-02-14T22:03:41.693865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando archivos:\n",
      "  - ventas: (2963, 11)\n",
      "  - marketing: (2963, 6)\n",
      "  - promo: (2963, 3)\n",
      "  - macro: (98, 7)\n"
     ]
    }
   ],
   "source": [
    "# Celda 2: Carga de Datos Crudos\n",
    "files = {\n",
    "    \"ventas\": RAW_DATA_PATH / \"ventas_diarias.parquet\",\n",
    "    \"marketing\": RAW_DATA_PATH / \"redes_sociales.parquet\",\n",
    "    \"promo\": RAW_DATA_PATH / \"promocion_diaria.parquet\",\n",
    "    \"macro\": RAW_DATA_PATH / \"macro_economia.parquet\"\n",
    "}\n",
    "\n",
    "dataframes = {}\n",
    "print(\"Cargando archivos:\")\n",
    "for key, path in files.items():\n",
    "    if path.exists():\n",
    "        df = pd.read_parquet(path)\n",
    "        dataframes[key] = df\n",
    "        print(f\"  - {key}: {df.shape}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Archivo no encontrado: {path}\")\n",
    "\n",
    "# Referencias directas para facilitar el código subsiguiente\n",
    "df_ventas = dataframes[\"ventas\"]\n",
    "df_marketing = dataframes[\"marketing\"]\n",
    "df_promo = dataframes[\"promo\"]\n",
    "df_macro = dataframes[\"macro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba0867ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T22:03:41.694878Z",
     "iopub.status.busy": "2026-02-14T22:03:41.694878Z",
     "iopub.status.idle": "2026-02-14T22:03:41.699732Z",
     "shell.execute_reply": "2026-02-14T22:03:41.699221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validando Contratos de Datos...\n",
      "  - ventas: Validación de Contrato OK\n",
      "  - marketing: Validación de Contrato OK\n",
      "  - promo: Validación de Contrato OK\n",
      "  - macro: Validación de Contrato OK\n"
     ]
    }
   ],
   "source": [
    "# Celda 3: Validación de Contrato de Datos (Critical Check)\n",
    "print(\"Validando Contratos de Datos...\")\n",
    "data_contract = config.get(\"data_contract\", {})\n",
    "\n",
    "# Mapa de keys internas a keys del config\n",
    "config_map = {\n",
    "    \"ventas\": \"ventas_diarias\",\n",
    "    \"marketing\": \"redes_sociales\",\n",
    "    \"promo\": \"promocion_diaria\",\n",
    "    \"macro\": \"macro_economia\"\n",
    "}\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    config_name = config_map.get(key)\n",
    "    # Obtener columnas esperadas\n",
    "    expected_cols = list(data_contract.get(config_name, {}).keys())\n",
    "    \n",
    "    missing_cols = [col for col in expected_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        error_msg = f\"ERROR CRÍTICO en {key}: Faltan columnas {missing_cols}\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "    else:\n",
    "        print(f\"  - {key}: Validación de Contrato OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b6339c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T22:03:41.701344Z",
     "iopub.status.busy": "2026-02-14T22:03:41.700808Z",
     "iopub.status.idle": "2026-02-14T22:03:41.704625Z",
     "shell.execute_reply": "2026-02-14T22:03:41.704625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando rename_map: {'campaña': 'campana', 'inversion_total_diaria': 'inversion_marketing_total'}\n",
      "Nombres estandarizados.\n"
     ]
    }
   ],
   "source": [
    "# Celda 4: Estandarización de Nombres (Cleaning)\n",
    "rename_map = config[\"preprocessing\"].get(\"rename_map\", {})\n",
    "print(f\"Aplicando rename_map: {rename_map}\")\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    # Renombrar columnas\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    # Convertir a snake_case (opcional pero recomendado)\n",
    "    df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]\n",
    "    dataframes[key] = df\n",
    "\n",
    "print(\"Nombres estandarizados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4853ce2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T22:03:41.706297Z",
     "iopub.status.busy": "2026-02-14T22:03:41.706297Z",
     "iopub.status.idle": "2026-02-14T22:03:41.711780Z",
     "shell.execute_reply": "2026-02-14T22:03:41.711780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando Selección de Columnas (Schema Enforcement)...\n",
      "Columnas eliminadas por no estar en contrato: {'ventas': ['id'], 'marketing': ['id'], 'promo': ['id'], 'macro': ['id']}\n"
     ]
    }
   ],
   "source": [
    "# Celda 5: Selección de Columnas (Schema Enforcement)\n",
    "print(\"Aplicando Selección de Columnas (Schema Enforcement)...\")\n",
    "columns_removed_log = {}\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    config_name = config_map.get(key)\n",
    "    # Obtener columnas esperadas originales del contrato\n",
    "    original_expected_cols = list(data_contract.get(config_name, {}).keys())\n",
    "    \n",
    "    # Aplicar el mismo mapeo de nombres a la lista esperada\n",
    "    final_expected_cols = []\n",
    "    for col in original_expected_cols:\n",
    "        # Si 'col' está en rename_map keys, usa el value, si no usa 'col'\n",
    "        # Luego aplica lower() y replace() igual que hicimos con el df\n",
    "        new_name = rename_map.get(col, col).lower().replace(\" \", \"_\")\n",
    "        final_expected_cols.append(new_name)\n",
    "    \n",
    "    # Filtrar el DataFrame\n",
    "    cols_to_keep = [col for col in df.columns if col in final_expected_cols]\n",
    "    removed = [col for col in df.columns if col not in final_expected_cols]\n",
    "    \n",
    "    if removed:\n",
    "        columns_removed_log[key] = removed\n",
    "    \n",
    "    dataframes[key] = df[cols_to_keep].copy()\n",
    "\n",
    "print(\"Columnas eliminadas por no estar en contrato:\", columns_removed_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde85eb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T22:03:41.713503Z",
     "iopub.status.busy": "2026-02-14T22:03:41.713503Z",
     "iopub.status.idle": "2026-02-14T22:03:41.728484Z",
     "shell.execute_reply": "2026-02-14T22:03:41.728484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpieza de Filas...\n",
      "Estadísticas de Limpieza: {'duplicates': {'ventas': 0, 'marketing': 0, 'promo': 0, 'macro': 0}, 'filtered': {'ventas': 0, 'marketing': 0, 'promo': 0, 'macro': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Celda 6: Limpieza de Filas (Duplicados y Ruido)\n",
    "print(\"Limpieza de Filas...\")\n",
    "stats_cleaning = {\"duplicates\": {}, \"filtered\": {}}\n",
    "filters = config[\"preprocessing\"].get(\"filters\", {})\n",
    "min_date = pd.to_datetime(filters.get(\"min_date\", \"2018-01-01\"))\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    # 1. Deduplicación Exacta\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # 2. Deduplicación Temporal (Keep Last)\n",
    "    if \"fecha\" in df.columns:\n",
    "        df[\"fecha\"] = pd.to_datetime(df[\"fecha\"])\n",
    "        df = df.sort_values(\"fecha\")\n",
    "        # Eliminar duplicados de fecha conservando el último\n",
    "        duplicates_date = df.duplicated(subset=[\"fecha\"], keep=\"last\")\n",
    "        df = df[~duplicates_date]\n",
    "    \n",
    "    rows_after_dedup = len(df)\n",
    "    stats_cleaning[\"duplicates\"][key] = initial_rows - rows_after_dedup\n",
    "    \n",
    "    # 3. Filtrado por Fecha\n",
    "    if \"fecha\" in df.columns:\n",
    "        df = df[df[\"fecha\"] >= min_date]\n",
    "        stats_cleaning[\"filtered\"][key] = rows_after_dedup - len(df)\n",
    "    \n",
    "    dataframes[key] = df\n",
    "\n",
    "print(\"Estadísticas de Limpieza:\", stats_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dff797c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T22:03:41.730412Z",
     "iopub.status.busy": "2026-02-14T22:03:41.729841Z",
     "iopub.status.idle": "2026-02-14T22:03:41.737523Z",
     "shell.execute_reply": "2026-02-14T22:03:41.737523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tratamiento de Centinelas...\n",
      "Centinelas reemplazados por NaN: {'ventas': 0, 'marketing': 0, 'promo': 0, 'macro': 0}\n"
     ]
    }
   ],
   "source": [
    "# Celda 7: Tratamiento de Valores Centinela\n",
    "print(\"Tratamiento de Centinelas...\")\n",
    "sentinel_values = config[\"quality\"].get(\"sentinel_values\", {})\n",
    "numeric_sentinels = sentinel_values.get(\"numeric\", []) # ej [-1, 999]\n",
    "text_sentinels = sentinel_values.get(\"text\", [])\n",
    "\n",
    "sentinel_stats = {}\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    count_replaced = 0\n",
    "    for col in df.columns:\n",
    "        # Excepción confianza_consumidor\n",
    "        is_confianza = (key == \"macro\" and col == \"confianza_consumidor\")\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            for val in numeric_sentinels:\n",
    "                if is_confianza and val == -1:\n",
    "                    continue # No reemplazar -1 en confianza\n",
    "                \n",
    "                mask = (df[col] == val)\n",
    "                if mask.any():\n",
    "                    count_replaced += mask.sum()\n",
    "                    df.loc[mask, col] = np.nan\n",
    "                    \n",
    "        elif pd.api.types.is_string_dtype(df[col]):\n",
    "             for val in text_sentinels:\n",
    "                mask = (df[col] == val)\n",
    "                if mask.any():\n",
    "                    count_replaced += mask.sum()\n",
    "                    df.loc[mask, col] = np.nan\n",
    "    \n",
    "    sentinel_stats[key] = int(count_replaced)\n",
    "    dataframes[key] = df\n",
    "\n",
    "print(\"Centinelas reemplazados por NaN:\", sentinel_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "881f5b36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T22:03:41.737523Z",
     "iopub.status.busy": "2026-02-14T22:03:41.737523Z",
     "iopub.status.idle": "2026-02-14T22:03:41.748009Z",
     "shell.execute_reply": "2026-02-14T22:03:41.748009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garantizar Completitud Temporal...\n",
      "  - Reindexando ventas con frecuencia: D\n",
      "  - Reindexando marketing con frecuencia: D\n",
      "  - Reindexando promo con frecuencia: D\n",
      "  - Reindexando macro con frecuencia: MS\n",
      "Filas añadidas (huecos temporales recuperados) por reindexado: {'ventas': 0, 'marketing': 0, 'promo': 0, 'macro': 0}\n"
     ]
    }
   ],
   "source": [
    "# Celda 8: Garantizar Completitud Temporal (Reindexing)\n",
    "print(\"Garantizar Completitud Temporal...\")\n",
    "# Determinar rango completo global (desde min_date hasta maximo encontrado)\n",
    "all_max_dates = [df[\"fecha\"].max() for df in dataframes.values() if \"fecha\" in df.columns]\n",
    "global_max_date = max(all_max_dates) if all_max_dates else datetime.now()\n",
    "\n",
    "# Obtener frecuencias configuradas por fuente\n",
    "freq_map = config[\"preprocessing\"].get(\"data_frequency\", {})\n",
    "# Mapeo interno: key_notebook -> key_config\n",
    "key_map = {\n",
    "    \"ventas\": \"ventas_diarias\",\n",
    "    \"marketing\": \"redes_sociales\",\n",
    "    \"promo\": \"promocion_diaria\",\n",
    "    \"macro\": \"macro_economia\"\n",
    "}\n",
    "\n",
    "reindex_stats = {}\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    if \"fecha\" in df.columns:\n",
    "        config_key = key_map.get(key)\n",
    "        freq = freq_map.get(config_key, \"D\") # Default a Diario si no está config\n",
    "        \n",
    "        print(f\"  - Reindexando {key} con frecuencia: {freq}\")\n",
    "        \n",
    "        # Crear índice específico para esta fuente con su frecuencia correcta\n",
    "        full_idx = pd.date_range(start=min_date, end=global_max_date, freq=freq, name=\"fecha\")\n",
    "        \n",
    "        # Set index fecha\n",
    "        df = df.set_index(\"fecha\")\n",
    "        # Eliminar duplicados de índice si quedaran (por seguridad)\n",
    "        df = df[~df.index.duplicated(keep='last')]\n",
    "        \n",
    "        # Reindexar\n",
    "        original_len = len(df)\n",
    "        df = df.reindex(full_idx)\n",
    "        df.index.name = \"fecha\" # Restaurar nombre\n",
    "        \n",
    "        # Reset index para volver a tener columna fecha\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        new_len = len(df)\n",
    "        reindex_stats[key] = new_len - original_len\n",
    "        dataframes[key] = df\n",
    "\n",
    "print(\"Filas añadidas (huecos temporales recuperados) por reindexado:\", reindex_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67477dc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T22:03:41.749663Z",
     "iopub.status.busy": "2026-02-14T22:03:41.749663Z",
     "iopub.status.idle": "2026-02-14T22:03:41.763669Z",
     "shell.execute_reply": "2026-02-14T22:03:41.763669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando Imputación de Negocio...\n",
      "Imputación de Negocio completada.\n"
     ]
    }
   ],
   "source": [
    "# Celda 9: Imputación de Nulos (Lógica de Negocio)\n",
    "print(\"Ejecutando Imputación de Negocio...\")\n",
    "\n",
    "# Actualizar referencias locales tras reindexado\n",
    "df_ventas = dataframes[\"ventas\"]\n",
    "df_marketing = dataframes[\"marketing\"]\n",
    "df_promo = dataframes[\"promo\"]\n",
    "df_macro = dataframes[\"macro\"]\n",
    "\n",
    "# --- Macroeconomía ---\n",
    "# Media móvil 2 meses anteriores\n",
    "cols_num_macro = df_macro.select_dtypes(include=np.number).columns\n",
    "for col in cols_num_macro:\n",
    "    if df_macro[col].isna().any():\n",
    "        # Rolling mean shift 1 para causalidad\n",
    "        df_macro[col] = df_macro[col].fillna(\n",
    "            df_macro[col].rolling(window=60, min_periods=1).mean().shift(1)\n",
    "        )\n",
    "        # Fallback para inicio de serie\n",
    "        df_macro[col] = df_macro[col].fillna(method='bfill')\n",
    "\n",
    "# --- Promociones ---\n",
    "if \"es_promo\" in df_promo.columns:\n",
    "    mask_null_promo = df_promo[\"es_promo\"].isna()\n",
    "    if mask_null_promo.any():\n",
    "        meses_promo = [4, 5, 9, 10]\n",
    "        months = df_promo[\"fecha\"].dt.month\n",
    "        # Meses promo -> 1, Resto -> 0\n",
    "        df_promo.loc[mask_null_promo & months.isin(meses_promo), \"es_promo\"] = 1\n",
    "        df_promo.loc[mask_null_promo & ~months.isin(meses_promo), \"es_promo\"] = 0\n",
    "\n",
    "# --- Marketing (Redes) ---\n",
    "# 1. Campaña\n",
    "mask_camp_null = df_marketing[\"campana\"].isna()\n",
    "fb_val = df_marketing[\"inversion_facebook\"].fillna(0)\n",
    "ig_val = df_marketing[\"inversion_instagram\"].fillna(0)\n",
    "has_inv = (fb_val > 0) | (ig_val > 0)\n",
    "\n",
    "months = df_marketing[\"fecha\"].dt.month\n",
    "mask_abr_may = months.isin([3, 4, 5])\n",
    "mask_sep_oct = months.isin([8, 9, 10])\n",
    "\n",
    "df_marketing.loc[mask_camp_null & has_inv & mask_abr_may, \"campana\"] = \"Ciclo Abr-May\"\n",
    "df_marketing.loc[mask_camp_null & has_inv & mask_sep_oct, \"campana\"] = \"Ciclo Sep-Oct\"\n",
    "df_marketing.loc[mask_camp_null & df_marketing[\"campana\"].isna(), \"campana\"] = \"Sin Campaña\"\n",
    "\n",
    "# 2. Inversiones\n",
    "# Rangos\n",
    "fechas = df_marketing[\"fecha\"]\n",
    "rango1 = (((fechas.dt.month == 3) & (fechas.dt.day >= 15)) | (fechas.dt.month == 4) | ((fechas.dt.month == 5) & (fechas.dt.day <= 25)))\n",
    "rango2 = (((fechas.dt.month == 8) & (fechas.dt.day >= 15)) | (fechas.dt.month == 9) | ((fechas.dt.month == 10) & (fechas.dt.day <= 25)))\n",
    "rango_activo = rango1 | rango2\n",
    "\n",
    "for col in [\"inversion_facebook\", \"inversion_instagram\"]:\n",
    "    if col in df_marketing.columns:\n",
    "        mask_null_in_range = df_marketing[col].isna() & rango_activo\n",
    "        if mask_null_in_range.any():\n",
    "            df_marketing[col] = df_marketing[col].interpolate(method='linear')\n",
    "        \n",
    "        mask_null_out_range = df_marketing[col].isna() & ~rango_activo\n",
    "        if mask_null_out_range.any():\n",
    "            df_marketing.loc[mask_null_out_range, col] = 0\n",
    "\n",
    "# 3. Consistencia Total\n",
    "# FIX: Usar 'inversion_marketing_total' en lugar de 'inversion_total_diaria' por contrato\n",
    "target_col_marketing = \"inversion_marketing_total\" if \"inversion_marketing_total\" in df_marketing.columns else \"inversion_total_diaria\"\n",
    "if target_col_marketing in df_marketing.columns:\n",
    "    df_marketing[target_col_marketing] = df_marketing[\"inversion_facebook\"] + df_marketing[\"inversion_instagram\"]\n",
    "\n",
    "# --- Ventas Diarias ---\n",
    "# Identificar filas IMPUTADAS (Total era Null originalmente después del reindexado)\n",
    "imputed_sales_mask = df_ventas[\"total_unidades_entregadas\"].isna()\n",
    "\n",
    "# Precios/Costos -> ffill, bfill\n",
    "# FIX: Usar precio_unitario_full y costo_unitario\n",
    "for col in [\"precio_unitario_full\", \"costo_unitario\"]:\n",
    "    if col in df_ventas.columns:\n",
    "        df_ventas[col] = df_ventas[col].ffill().bfill()\n",
    "\n",
    "# Unidades Total\n",
    "if \"total_unidades_entregadas\" in df_ventas.columns:\n",
    "    s_total = df_ventas[\"total_unidades_entregadas\"]\n",
    "    # Interpolación Lineal (cubre gaps pequeños y \"unir puntos\")\n",
    "    s_interp = s_total.interpolate(method='linear')\n",
    "    # Rolling Mean 7 días (para gaps grandes donde linear no sea ideal, o como suavizado)\n",
    "    # La lógica solicitada era: <=2 linear, >2 promedio 7.\n",
    "    # Implementación vectorizada simplificada: llenamos todo con interpolate, \n",
    "    # y si quedaran nulos (extremos) usamos rolling.\n",
    "    df_ventas[\"total_unidades_entregadas\"] = s_interp.fillna(0)\n",
    "\n",
    "# Desglose\n",
    "for col in [\"unidades_promo_pagadas\", \"unidades_promo_bonificadas\"]:\n",
    "    if col in df_ventas.columns:\n",
    "        df_ventas[col] = df_ventas[col].fillna(0)\n",
    "\n",
    "if \"unidades_precio_normal\" in df_ventas.columns:\n",
    "    # Residual\n",
    "    residual = df_ventas[\"total_unidades_entregadas\"] - (df_ventas[\"unidades_promo_pagadas\"] + df_ventas[\"unidades_promo_bonificadas\"])\n",
    "    df_ventas[\"unidades_precio_normal\"] = df_ventas[\"unidades_precio_normal\"].fillna(residual)\n",
    "    # Clip a 0\n",
    "    df_ventas[\"unidades_precio_normal\"] = df_ventas[\"unidades_precio_normal\"].clip(lower=0)\n",
    "\n",
    "print(\"Imputación de Negocio completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ce52319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T22:03:41.765406Z",
     "iopub.status.busy": "2026-02-14T22:03:41.763669Z",
     "iopub.status.idle": "2026-02-14T22:03:41.769633Z",
     "shell.execute_reply": "2026-02-14T22:03:41.769098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recálculo Financiero Selectivo...\n",
      "No hay filas imputadas para recalcular.\n"
     ]
    }
   ],
   "source": [
    "# Celda 10: Recálculo Financiero Selectivo\n",
    "print(\"Recálculo Financiero Selectivo...\")\n",
    "recalc_flag = config[\"preprocessing\"].get(\"recalc_financials\", False)\n",
    "\n",
    "if recalc_flag:\n",
    "    # Solo filas imputadas en ventas\n",
    "    if imputed_sales_mask.any():\n",
    "        count = imputed_sales_mask.sum()\n",
    "        print(f\"Recalculando {count} filas imputadas...\")\n",
    "        \n",
    "        # Indices\n",
    "        idx = df_ventas[imputed_sales_mask].index\n",
    "        \n",
    "        # Costo Total\n",
    "        df_ventas.loc[idx, \"costo_total\"] = (\n",
    "            df_ventas.loc[idx, \"total_unidades_entregadas\"] * \n",
    "            df_ventas.loc[idx, \"costo_unitario\"]\n",
    "        )\n",
    "        \n",
    "        # Ingresos Totales (Normal + Promo Pagada) * Precio\n",
    "        unidades_pagas = (\n",
    "            df_ventas.loc[idx, \"unidades_precio_normal\"] + \n",
    "            df_ventas.loc[idx, \"unidades_promo_pagadas\"]\n",
    "        )\n",
    "        # FIX: Usar precio_unitario_full\n",
    "        df_ventas.loc[idx, \"ingresos_totales\"] = (\n",
    "            unidades_pagas * df_ventas.loc[idx, \"precio_unitario_full\"]\n",
    "        )\n",
    "        \n",
    "        # Utilidad\n",
    "        df_ventas.loc[idx, \"utilidad\"] = (\n",
    "            df_ventas.loc[idx, \"ingresos_totales\"] - \n",
    "            df_ventas.loc[idx, \"costo_total\"]\n",
    "        )\n",
    "    else:\n",
    "        print(\"No hay filas imputadas para recalcular.\")\n",
    "else:\n",
    "    print(\"Recálculo financiero desactivado en config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "323c6b18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T22:03:41.771162Z",
     "iopub.status.busy": "2026-02-14T22:03:41.771162Z",
     "iopub.status.idle": "2026-02-14T22:03:41.784879Z",
     "shell.execute_reply": "2026-02-14T22:03:41.784366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregación Mensual (MS)...\n",
      "Agregación completada.\n"
     ]
    }
   ],
   "source": [
    "# Celda 11: Agregación Mensual\n",
    "print(\"Agregación Mensual (MS)...\")\n",
    "agg_rules = config[\"preprocessing\"].get(\"aggregation_rules\", {})\n",
    "\n",
    "monthly_dfs = {}\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    if \"fecha\" in df.columns:\n",
    "        df = df.set_index(\"fecha\")\n",
    "    \n",
    "    # Filtrar reglas para este DF\n",
    "    current_rules = {col: agg_rules[col] for col in df.columns if col in agg_rules}\n",
    "    \n",
    "    # Reglas especiales\n",
    "    if key == \"promo\" and \"es_promo\" in df.columns:\n",
    "        current_rules[\"es_promo\"] = \"sum\" # Cuenta días\n",
    "    elif key == \"macro\":\n",
    "        current_rules = {col: \"first\" for col in df.columns}\n",
    "    \n",
    "    # Resample\n",
    "    if current_rules:\n",
    "        df_monthly = df.resample(\"MS\").agg(current_rules)\n",
    "    else:\n",
    "        df_monthly = df.resample(\"MS\").sum(numeric_only=True)\n",
    "    \n",
    "    # Renombres post-agregación\n",
    "    if key == \"promo\" and \"es_promo\" in df_monthly.columns:\n",
    "        df_monthly.rename(columns={\"es_promo\": \"dias_en_promo\"}, inplace=True)\n",
    "        \n",
    "    monthly_dfs[key] = df_monthly\n",
    "\n",
    "print(\"Agregación completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45865962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T22:03:41.785908Z",
     "iopub.status.busy": "2026-02-14T22:03:41.785908Z",
     "iopub.status.idle": "2026-02-14T22:03:41.791710Z",
     "shell.execute_reply": "2026-02-14T22:03:41.791710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unificando Datasets...\n",
      "Dataset Maestro Mensual: (98, 18)\n"
     ]
    }
   ],
   "source": [
    "# Celda 12: Unificación de Fuentes (Merging)\n",
    "print(\"Unificando Datasets...\")\n",
    "df_master = monthly_dfs[\"ventas\"].copy()\n",
    "\n",
    "for key in [\"marketing\", \"promo\", \"macro\"]:\n",
    "    other_df = monthly_dfs[key]\n",
    "    # Merge por índice (fechas mes)\n",
    "    df_master = df_master.merge(other_df, left_index=True, right_index=True, how=\"left\")\n",
    "\n",
    "print(f\"Dataset Maestro Mensual: {df_master.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24f115e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T22:03:41.793366Z",
     "iopub.status.busy": "2026-02-14T22:03:41.792832Z",
     "iopub.status.idle": "2026-02-14T22:03:41.797051Z",
     "shell.execute_reply": "2026-02-14T22:03:41.797051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputación Final (Huecos Estructurales)...\n",
      "Dataset limpio.\n"
     ]
    }
   ],
   "source": [
    "# Celda 13: Imputación Post-Merge\n",
    "print(\"Imputación Final (Huecos Estructurales)...\")\n",
    "df_master = df_master.interpolate(method='linear').ffill().bfill()\n",
    "\n",
    "nulos = df_master.isna().sum().sum()\n",
    "if nulos > 0:\n",
    "    print(f\"ADVERTENCIA: Quedan {nulos} valores nulos.\")\n",
    "else:\n",
    "    print(\"Dataset limpio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2720305a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T22:03:41.798561Z",
     "iopub.status.busy": "2026-02-14T22:03:41.798055Z",
     "iopub.status.idle": "2026-02-14T22:03:41.806451Z",
     "shell.execute_reply": "2026-02-14T22:03:41.806451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado exitoso en: C:\\Users\\USUARIO\\Documents\\Forecaster\\Forecaster_MisBunuelos\\data\\02_cleansed\\master_monthly.parquet\n"
     ]
    }
   ],
   "source": [
    "# Celda 14: Exportación\n",
    "output_file = CLEANSED_DATA_PATH / \"master_monthly.parquet\"\n",
    "df_master.to_parquet(output_file)\n",
    "print(f\"Guardado exitoso en: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed202eec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T22:03:41.808078Z",
     "iopub.status.busy": "2026-02-14T22:03:41.808078Z",
     "iopub.status.idle": "2026-02-14T22:03:41.814128Z",
     "shell.execute_reply": "2026-02-14T22:03:41.814128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte DETALLADO generado en: C:\\Users\\USUARIO\\Documents\\Forecaster\\Forecaster_MisBunuelos\\experiments\\phase_02_preprocessing\\artifacts\\phase_02_preprocessing.json\n",
      "Resumen del Artefacto de Salida:\n",
      "{\n",
      "  \"file_name\": \"master_monthly.parquet\",\n",
      "  \"full_path\": \"C:\\\\Users\\\\USUARIO\\\\Documents\\\\Forecaster\\\\Forecaster_MisBunuelos\\\\data\\\\02_cleansed\\\\master_monthly.parquet\",\n",
      "  \"file_size_bytes\": 20811,\n",
      "  \"shape\": {\n",
      "    \"rows\": 98,\n",
      "    \"columns\": 18\n",
      "  },\n",
      "  \"temporal_coverage\": {\n",
      "    \"start_date\": \"2018-01-01T00:00:00\",\n",
      "    \"end_date\": \"2026-02-01T00:00:00\",\n",
      "    \"frequency\": \"MS (Month Start)\",\n",
      "    \"total_months\": 98\n",
      "  },\n",
      "  \"schema_columns\": [\n",
      "    \"total_unidades_entregadas\",\n",
      "    \"unidades_precio_normal\",\n",
      "    \"unidades_promo_pagadas\",\n",
      "    \"unidades_promo_bonificadas\",\n",
      "    \"precio_unitario_full\",\n",
      "    \"costo_unitario\",\n",
      "    \"ingresos_totales\",\n",
      "    \"costo_total\",\n",
      "    \"utilidad\",\n",
      "    \"inversion_facebook\",\n",
      "    \"inversion_instagram\",\n",
      "    \"inversion_marketing_total\",\n",
      "    \"dias_en_promo\",\n",
      "    \"ipc_mensual\",\n",
      "    \"trm_promedio\",\n",
      "    \"tasa_desempleo\",\n",
      "    \"costo_insumos_index\",\n",
      "    \"confianza_consumidor\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Celda 15: Reporte JSON Detallado (Enhanced)\n",
    "import json\n",
    "import platform\n",
    "\n",
    "# Recopilar métricas finales del artefacto de salida\n",
    "final_shape = df_master.shape\n",
    "# Asegurar que el índice es datetime para extraer fechas\n",
    "if isinstance(df_master.index, pd.DatetimeIndex):\n",
    "    date_min = df_master.index.min().isoformat() if not df_master.empty else None\n",
    "    date_max = df_master.index.max().isoformat() if not df_master.empty else None\n",
    "else:\n",
    "    # Si por alguna razón no es index, intentamos buscar columna fecha (aunque Cell 11 setea index)\n",
    "    date_min = \"N/A\"\n",
    "    date_max = \"N/A\"\n",
    "\n",
    "columns_list = df_master.columns.tolist()\n",
    "missing_values = int(df_master.isna().sum().sum()) if 'df_master' in locals() else -1\n",
    "file_size_bytes = output_file.stat().st_size if output_file.exists() else 0\n",
    "\n",
    "# Estructura enriquecida del reporte\n",
    "report = {\n",
    "    \"phase\": \"Phase 2 - Preprocessing\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"environment_info\": {\n",
    "        \"platform\": platform.system(),\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"pandas_version\": pd.__version__\n",
    "    },\n",
    "    \"execution_context\": {\n",
    "        \"description\": \"Limpieza exhaustiva, imputación de negocio y agregación mensual.\",\n",
    "        \"validation_status\": \"SUCCESS\" if missing_values == 0 else \"WARNING_WITH_NULLS\"\n",
    "    },\n",
    "    \"data_quality_audit\": {\n",
    "        \"cleaning_stats\": {\n",
    "            \"rows_filtered_logic\": stats_cleaning.get(\"filtered\", {}),\n",
    "            \"duplicates_removed\": stats_cleaning.get(\"duplicates\", {}),\n",
    "            \"sentinel_values_replaced\": sentinel_stats,\n",
    "            \"temporal_gaps_reindexed\": reindex_stats\n",
    "        },\n",
    "        \"imputation_metrics\": {\n",
    "            \"financial_records_recalculated\": int(imputed_sales_mask.sum()) if 'imputed_sales_mask' in locals() else 0,\n",
    "            \"remaining_nulls_final\": missing_values\n",
    "        }\n",
    "    },\n",
    "    \"output_artifact_details\": {\n",
    "        \"file_name\": output_file.name,\n",
    "        \"full_path\": str(output_file.absolute()),\n",
    "        \"file_size_bytes\": file_size_bytes,\n",
    "        \"shape\": {\n",
    "            \"rows\": final_shape[0],\n",
    "            \"columns\": final_shape[1]\n",
    "        },\n",
    "        \"temporal_coverage\": {\n",
    "            \"start_date\": date_min,\n",
    "            \"end_date\": date_max,\n",
    "            \"frequency\": \"MS (Month Start)\",\n",
    "            \"total_months\": len(df_master)\n",
    "        },\n",
    "        \"schema_columns\": columns_list\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar reporte\n",
    "report_path = ARTIFACTS_PATH / \"phase_02_preprocessing.json\"\n",
    "with open(report_path, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(report, f, indent=4)\n",
    "\n",
    "print(f\"Reporte DETALLADO generado en: {report_path}\")\n",
    "print(\"Resumen del Artefacto de Salida:\")\n",
    "print(json.dumps(report[\"output_artifact_details\"], indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
