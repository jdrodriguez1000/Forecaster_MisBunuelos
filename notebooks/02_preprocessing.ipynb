{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb89beb",
   "metadata": {},
   "source": [
    "# Fase 2: Robust Data Preprocessing\n",
    "\n",
    "Este notebook implementa el pipeline de limpieza, validación, imputación y agregación mensual de los datos crudos para el proyecto **Forecaster Mis Buñuelos**.\n",
    "\n",
    "**Objetivo:** Generar `data/02_cleansed/master_monthly.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ed4689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.067781Z",
     "iopub.status.busy": "2026-02-15T17:01:31.067781Z",
     "iopub.status.idle": "2026-02-15T17:01:31.678182Z",
     "shell.execute_reply": "2026-02-15T17:01:31.677641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración cargada y rutas establecidas.\n"
     ]
    }
   ],
   "source": [
    "# Celda 1: Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "import platform\n",
    "\n",
    "# Configurar pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Definir Rutas\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "if BASE_DIR.name == \"notebooks\":\n",
    "    BASE_DIR = BASE_DIR.parent\n",
    "\n",
    "CONFIG_PATH = BASE_DIR / \"config.yaml\"\n",
    "RAW_DATA_PATH = BASE_DIR / \"data\" / \"01_raw\"\n",
    "CLEANSED_DATA_PATH = BASE_DIR / \"data\" / \"02_cleansed\"\n",
    "ARTIFACTS_PATH = BASE_DIR / \"experiments\" / \"phase_02_preprocessing\" / \"artifacts\"\n",
    "\n",
    "# Crear directorios\n",
    "CLEANSED_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cargar Configuración\n",
    "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuración cargada y rutas establecidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97a1ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.681216Z",
     "iopub.status.busy": "2026-02-15T17:01:31.681216Z",
     "iopub.status.idle": "2026-02-15T17:01:31.739278Z",
     "shell.execute_reply": "2026-02-15T17:01:31.738726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando archivos:\n",
      "  - ventas: (2963, 11)\n",
      "  - marketing: (2963, 6)\n",
      "  - promo: (2963, 3)\n",
      "  - macro: (98, 7)\n"
     ]
    }
   ],
   "source": [
    "# Celda 2: Carga de Datos Crudos\n",
    "files = {\n",
    "    \"ventas\": RAW_DATA_PATH / \"ventas_diarias.parquet\",\n",
    "    \"marketing\": RAW_DATA_PATH / \"redes_sociales.parquet\",\n",
    "    \"promo\": RAW_DATA_PATH / \"promocion_diaria.parquet\",\n",
    "    \"macro\": RAW_DATA_PATH / \"macro_economia.parquet\"\n",
    "}\n",
    "\n",
    "dataframes = {}\n",
    "print(\"Cargando archivos:\")\n",
    "for key, path in files.items():\n",
    "    if path.exists():\n",
    "        df = pd.read_parquet(path)\n",
    "        dataframes[key] = df\n",
    "        print(f\"  - {key}: {df.shape}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Archivo no encontrado: {path}\")\n",
    "\n",
    "# Referencias directas para facilitar el código subsiguiente\n",
    "df_ventas = dataframes[\"ventas\"]\n",
    "df_marketing = dataframes[\"marketing\"]\n",
    "df_promo = dataframes[\"promo\"]\n",
    "df_macro = dataframes[\"macro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df71815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.742269Z",
     "iopub.status.busy": "2026-02-15T17:01:31.740910Z",
     "iopub.status.idle": "2026-02-15T17:01:31.747504Z",
     "shell.execute_reply": "2026-02-15T17:01:31.747504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validando Contratos de Datos...\n",
      "  - ventas: Validación de Contrato OK\n",
      "  - marketing: Validación de Contrato OK\n",
      "  - promo: Validación de Contrato OK\n",
      "  - macro: Validación de Contrato OK\n"
     ]
    }
   ],
   "source": [
    "# Celda 3: Validación de Contrato de Datos (Critical Check)\n",
    "print(\"Validando Contratos de Datos...\")\n",
    "data_contract = config.get(\"data_contract\", {})\n",
    "\n",
    "# Mapa de keys internas a keys del config\n",
    "config_map = {\n",
    "    \"ventas\": \"ventas_diarias\",\n",
    "    \"marketing\": \"redes_sociales\",\n",
    "    \"promo\": \"promocion_diaria\",\n",
    "    \"macro\": \"macro_economia\"\n",
    "}\n",
    "\n",
    "data_contract_status = {}\n",
    "    \n",
    "for key, df in dataframes.items():\n",
    "    config_name = config_map.get(key)\n",
    "    # Obtener columnas esperadas\n",
    "    expected_cols = list(data_contract.get(config_name, {}).keys())\n",
    "    \n",
    "    missing_cols = [col for col in expected_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        error_msg = f\"ERROR CRÍTICO en {key}: Faltan columnas {missing_cols}\"\n",
    "        print(error_msg)\n",
    "        data_contract_status[key] = f\"FAILED: Missing {missing_cols}\"\n",
    "        raise RuntimeError(error_msg)\n",
    "    else:\n",
    "        print(f\"  - {key}: Validación de Contrato OK\")\n",
    "        data_contract_status[key] = \"OK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "955a22ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.750103Z",
     "iopub.status.busy": "2026-02-15T17:01:31.750103Z",
     "iopub.status.idle": "2026-02-15T17:01:31.756257Z",
     "shell.execute_reply": "2026-02-15T17:01:31.755737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando rename_map: {}\n",
      "Nombres estandarizados.\n"
     ]
    }
   ],
   "source": [
    "# Celda 4: Estandarización de Nombres (Cleaning)\n",
    "rename_map = config[\"preprocessing\"].get(\"rename_map\") or {}\n",
    "print(f\"Aplicando rename_map: {rename_map}\")\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    # Renombrar columnas\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    # Convertir a snake_case (opcional pero recomendado)\n",
    "    df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]\n",
    "    dataframes[key] = df\n",
    "\n",
    "print(\"Nombres estandarizados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e00f4097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.759471Z",
     "iopub.status.busy": "2026-02-15T17:01:31.759471Z",
     "iopub.status.idle": "2026-02-15T17:01:31.768135Z",
     "shell.execute_reply": "2026-02-15T17:01:31.767450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando Selección de Columnas (Schema Enforcement)...\n",
      "Columnas eliminadas por no estar en contrato: {'ventas': ['id'], 'marketing': ['id'], 'promo': ['id'], 'macro': ['id']}\n"
     ]
    }
   ],
   "source": [
    "# Celda 5: Selección de Columnas (Schema Enforcement)\n",
    "print(\"Aplicando Selección de Columnas (Schema Enforcement)...\")\n",
    "columns_removed_log = {}\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    config_name = config_map.get(key)\n",
    "    # Obtener columnas esperadas originales del contrato\n",
    "    original_expected_cols = list(data_contract.get(config_name, {}).keys())\n",
    "    \n",
    "    # Aplicar el mismo mapeo de nombres a la lista esperada\n",
    "    final_expected_cols = []\n",
    "    for col in original_expected_cols:\n",
    "        # Si 'col' está en rename_map keys, usa el value, si no usa 'col'\n",
    "        # Luego aplica lower() y replace() igual que hicimos con el df\n",
    "        new_name = rename_map.get(col, col).lower().replace(\" \", \"_\")\n",
    "        final_expected_cols.append(new_name)\n",
    "    \n",
    "    # Filtrar el DataFrame\n",
    "    cols_to_keep = [col for col in df.columns if col in final_expected_cols]\n",
    "    removed = [col for col in df.columns if col not in final_expected_cols]\n",
    "    \n",
    "    if removed:\n",
    "        columns_removed_log[key] = removed\n",
    "    \n",
    "    dataframes[key] = df[cols_to_keep].copy()\n",
    "\n",
    "print(\"Columnas eliminadas por no estar en contrato:\", columns_removed_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9ef3239",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.770764Z",
     "iopub.status.busy": "2026-02-15T17:01:31.770764Z",
     "iopub.status.idle": "2026-02-15T17:01:31.792315Z",
     "shell.execute_reply": "2026-02-15T17:01:31.792315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpieza de Filas...\n",
      "Estadísticas de Limpieza: {'duplicates': {'ventas': 0, 'marketing': 0, 'promo': 0, 'macro': 0}, 'filtered': {'ventas': 0, 'marketing': 0, 'promo': 0, 'macro': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Celda 6: Limpieza de Filas (Duplicados y Ruido)\n",
    "print(\"Limpieza de Filas...\")\n",
    "stats_cleaning = {\"duplicates\": {}, \"filtered\": {}}\n",
    "filters = config[\"preprocessing\"].get(\"filters\", {})\n",
    "min_date = pd.to_datetime(filters.get(\"min_date\", \"2018-01-01\"))\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    initial_rows = len(df)\n",
    "    \n",
    "    # 1. Deduplicación Exacta\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # 2. Deduplicación Temporal (Keep Last)\n",
    "    if \"fecha\" in df.columns:\n",
    "        df[\"fecha\"] = pd.to_datetime(df[\"fecha\"])\n",
    "        df = df.sort_values(\"fecha\")\n",
    "        # Eliminar duplicados de fecha conservando el último\n",
    "        duplicates_date = df.duplicated(subset=[\"fecha\"], keep=\"last\")\n",
    "        df = df[~duplicates_date]\n",
    "    \n",
    "    rows_after_dedup = len(df)\n",
    "    stats_cleaning[\"duplicates\"][key] = initial_rows - rows_after_dedup\n",
    "    \n",
    "    # 3. Filtrado por Fecha\n",
    "    if \"fecha\" in df.columns:\n",
    "        df = df[df[\"fecha\"] >= min_date]\n",
    "        stats_cleaning[\"filtered\"][key] = rows_after_dedup - len(df)\n",
    "    \n",
    "    dataframes[key] = df\n",
    "\n",
    "print(\"Estadísticas de Limpieza:\", stats_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94f59468",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.795367Z",
     "iopub.status.busy": "2026-02-15T17:01:31.795367Z",
     "iopub.status.idle": "2026-02-15T17:01:31.804792Z",
     "shell.execute_reply": "2026-02-15T17:01:31.804274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tratamiento de Centinelas...\n",
      "Centinelas reemplazados por NaN: {'ventas': 0, 'marketing': 0, 'promo': 0, 'macro': 0}\n"
     ]
    }
   ],
   "source": [
    "# Celda 7: Tratamiento de Valores Centinela\n",
    "print(\"Tratamiento de Centinelas...\")\n",
    "sentinel_values = config[\"quality\"].get(\"sentinel_values\", {})\n",
    "numeric_sentinels = sentinel_values.get(\"numeric\", []) # ej [-1, 999]\n",
    "text_sentinels = sentinel_values.get(\"text\", [])\n",
    "\n",
    "sentinel_stats = {}\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    count_replaced = 0\n",
    "    for col in df.columns:\n",
    "        # Excepción confianza_consumidor\n",
    "        is_confianza = (key == \"macro\" and col == \"confianza_consumidor\")\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            for val in numeric_sentinels:\n",
    "                if is_confianza and val == -1:\n",
    "                    continue # No reemplazar -1 en confianza\n",
    "                \n",
    "                mask = (df[col] == val)\n",
    "                if mask.any():\n",
    "                    count_replaced += mask.sum()\n",
    "                    df.loc[mask, col] = np.nan\n",
    "                    \n",
    "        elif pd.api.types.is_string_dtype(df[col]):\n",
    "             for val in text_sentinels:\n",
    "                mask = (df[col] == val)\n",
    "                if mask.any():\n",
    "                    count_replaced += mask.sum()\n",
    "                    df.loc[mask, col] = np.nan\n",
    "    \n",
    "    sentinel_stats[key] = int(count_replaced)\n",
    "    dataframes[key] = df\n",
    "\n",
    "print(\"Centinelas reemplazados por NaN:\", sentinel_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2f6251a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.807970Z",
     "iopub.status.busy": "2026-02-15T17:01:31.807970Z",
     "iopub.status.idle": "2026-02-15T17:01:31.820895Z",
     "shell.execute_reply": "2026-02-15T17:01:31.820053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garantizar Completitud Temporal...\n",
      "  - Reindexando ventas con frecuencia: D\n",
      "  - Reindexando marketing con frecuencia: D\n",
      "  - Reindexando promo con frecuencia: D\n",
      "  - Reindexando macro con frecuencia: MS\n",
      "Filas añadidas (huecos temporales recuperados) por reindexado: {'ventas': 0, 'marketing': 0, 'promo': 0, 'macro': 0}\n"
     ]
    }
   ],
   "source": [
    "# Celda 8: Garantizar Completitud Temporal (Reindexing)\n",
    "print(\"Garantizar Completitud Temporal...\")\n",
    "# Determinar rango completo global (desde min_date hasta maximo encontrado)\n",
    "all_max_dates = [df[\"fecha\"].max() for df in dataframes.values() if \"fecha\" in df.columns]\n",
    "global_max_date = max(all_max_dates) if all_max_dates else datetime.now()\n",
    "\n",
    "# Obtener frecuencias configuradas por fuente\n",
    "freq_map = config[\"preprocessing\"].get(\"data_frequency\", {})\n",
    "# Mapeo interno: key_notebook -> key_config\n",
    "key_map = {\n",
    "    \"ventas\": \"ventas_diarias\",\n",
    "    \"marketing\": \"redes_sociales\",\n",
    "    \"promo\": \"promocion_diaria\",\n",
    "    \"macro\": \"macro_economia\"\n",
    "}\n",
    "\n",
    "reindex_stats = {}\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    if \"fecha\" in df.columns:\n",
    "        config_key = key_map.get(key)\n",
    "        freq = freq_map.get(config_key, \"D\") # Default a Diario si no está config\n",
    "        \n",
    "        print(f\"  - Reindexando {key} con frecuencia: {freq}\")\n",
    "        \n",
    "        # Crear índice específico para esta fuente con su frecuencia correcta\n",
    "        full_idx = pd.date_range(start=min_date, end=global_max_date, freq=freq, name=\"fecha\")\n",
    "        \n",
    "        # Set index fecha\n",
    "        df = df.set_index(\"fecha\")\n",
    "        # Eliminar duplicados de índice si quedaran (por seguridad)\n",
    "        df = df[~df.index.duplicated(keep='last')]\n",
    "        \n",
    "        # Reindexar\n",
    "        original_len = len(df)\n",
    "        df = df.reindex(full_idx)\n",
    "        df.index.name = \"fecha\" # Restaurar nombre\n",
    "        \n",
    "        # Reset index para volver a tener columna fecha\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        new_len = len(df)\n",
    "        reindex_stats[key] = new_len - original_len\n",
    "        dataframes[key] = df\n",
    "\n",
    "print(\"Filas añadidas (huecos temporales recuperados) por reindexado:\", reindex_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d482bfd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.821639Z",
     "iopub.status.busy": "2026-02-15T17:01:31.821639Z",
     "iopub.status.idle": "2026-02-15T17:01:31.842319Z",
     "shell.execute_reply": "2026-02-15T17:01:31.842319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando Imputación de Negocio...\n",
      "Imputación de Negocio completada.\n",
      "Stats Imputación: {'macro': {}, 'promo': {}, 'marketing': {}, 'ventas': {'dates_missing_imputed': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Celda 9: Imputación de Nulos (Lógica de Negocio)\n",
    "print(\"Ejecutando Imputación de Negocio...\")\n",
    "imputation_stats = {\n",
    "    \"macro\": {},\n",
    "    \"promo\": {},\n",
    "    \"marketing\": {},\n",
    "    \"ventas\": {}\n",
    "}\n",
    "\n",
    "# Actualizar referencias locales tras reindexado\n",
    "df_ventas = dataframes[\"ventas\"]\n",
    "df_marketing = dataframes[\"marketing\"]\n",
    "df_promo = dataframes[\"promo\"]\n",
    "df_macro = dataframes[\"macro\"]\n",
    "\n",
    "# --- Macroeconomía ---\n",
    "# Media móvil 2 meses anteriores\n",
    "cols_num_macro = df_macro.select_dtypes(include=np.number).columns\n",
    "for col in cols_num_macro:\n",
    "    nulls_before = df_macro[col].isna().sum()\n",
    "    if nulls_before > 0:\n",
    "        # Rolling mean shift 1 para causalidad\n",
    "        df_macro[col] = df_macro[col].fillna(\n",
    "            df_macro[col].rolling(window=60, min_periods=1).mean().shift(1)\n",
    "        )\n",
    "        # Fallback para inicio de serie\n",
    "        df_macro[col] = df_macro[col].fillna(method='bfill')\n",
    "        imputation_stats[\"macro\"][col] = int(nulls_before)\n",
    "\n",
    "# --- Promociones ---\n",
    "if \"es_promo\" in df_promo.columns:\n",
    "    mask_null_promo = df_promo[\"es_promo\"].isna()\n",
    "    count_promo_nulls = mask_null_promo.sum()\n",
    "    if count_promo_nulls > 0:\n",
    "        meses_promo = [4, 5, 9, 10]\n",
    "        months = df_promo[\"fecha\"].dt.month\n",
    "        # Meses promo -> 1, Resto -> 0\n",
    "        df_promo.loc[mask_null_promo & months.isin(meses_promo), \"es_promo\"] = 1\n",
    "        df_promo.loc[mask_null_promo & ~months.isin(meses_promo), \"es_promo\"] = 0\n",
    "        imputation_stats[\"promo\"][\"es_promo_inferred\"] = int(count_promo_nulls)\n",
    "\n",
    "# --- Marketing (Redes) ---\n",
    "# 1. Campaña\n",
    "target_col_campana = \"ciclo\" if \"ciclo\" in df_marketing.columns else \"campana\"\n",
    "mask_camp_null = df_marketing[target_col_campana].isna()\n",
    "count_campana_nulls = mask_camp_null.sum()\n",
    "    \n",
    "if count_campana_nulls > 0:\n",
    "    fb_val = df_marketing[\"inversion_facebook\"].fillna(0)\n",
    "    ig_val = df_marketing[\"inversion_instagram\"].fillna(0)\n",
    "    has_inv = (fb_val > 0) | (ig_val > 0)\n",
    "\n",
    "    months = df_marketing[\"fecha\"].dt.month\n",
    "    mask_abr_may = months.isin([3, 4, 5])\n",
    "    mask_sep_oct = months.isin([8, 9, 10])\n",
    "\n",
    "    df_marketing.loc[mask_camp_null & has_inv & mask_abr_may, target_col_campana] = \"Ciclo Abr-May\"\n",
    "    df_marketing.loc[mask_camp_null & has_inv & mask_sep_oct, target_col_campana] = \"Ciclo Sep-Oct\"\n",
    "    df_marketing.loc[mask_camp_null & df_marketing[target_col_campana].isna(), target_col_campana] = \"Sin Campaña\"\n",
    "    imputation_stats[\"marketing\"][\"campaigns_inferred\"] = int(count_campana_nulls)\n",
    "\n",
    "# 2. Inversiones\n",
    "fechas = df_marketing[\"fecha\"]\n",
    "rango1 = (((fechas.dt.month == 3) & (fechas.dt.day >= 15)) | (fechas.dt.month == 4) | ((fechas.dt.month == 5) & (fechas.dt.day <= 25)))\n",
    "rango2 = (((fechas.dt.month == 8) & (fechas.dt.day >= 15)) | (fechas.dt.month == 9) | ((fechas.dt.month == 10) & (fechas.dt.day <= 25)))\n",
    "rango_activo = rango1 | rango2\n",
    "\n",
    "for col in [\"inversion_facebook\", \"inversion_instagram\"]:\n",
    "    if col in df_marketing.columns:\n",
    "        mask_null = df_marketing[col].isna()\n",
    "        count_inv_nulls = mask_null.sum()\n",
    "        if count_inv_nulls > 0:\n",
    "            mask_null_in_range = mask_null & rango_activo\n",
    "            if mask_null_in_range.any():\n",
    "                df_marketing[col] = df_marketing[col].interpolate(method='linear')\n",
    "            \n",
    "            mask_null_out_range = mask_null & ~rango_activo\n",
    "            if mask_null_out_range.any():\n",
    "                df_marketing.loc[mask_null_out_range, col] = 0\n",
    "            \n",
    "            imputation_stats[\"marketing\"][f\"{col}_imputed\"] = int(count_inv_nulls)\n",
    "\n",
    "# 3. Consistencia Total\n",
    "target_col_marketing = \"inversion_marketing_total\" if \"inversion_marketing_total\" in df_marketing.columns else \"inversion_total_diaria\"\n",
    "if target_col_marketing in df_marketing.columns:\n",
    "    df_marketing[target_col_marketing] = df_marketing[\"inversion_facebook\"] + df_marketing[\"inversion_instagram\"]\n",
    "\n",
    "# --- Ventas Diarias ---\n",
    "# Identificar filas IMPUTADAS (Total era Null originalmente después del reindexado)\n",
    "imputed_sales_mask = df_ventas[\"total_unidades_entregadas\"].isna()\n",
    "imputation_stats[\"ventas\"][\"dates_missing_imputed\"] = int(imputed_sales_mask.sum())\n",
    "\n",
    "# Precios/Costos -> ffill, bfill\n",
    "for col in [\"precio_unitario_full\", \"costo_unitario\"]:\n",
    "    if col in df_ventas.columns:\n",
    "        nulls = df_ventas[col].isna().sum()\n",
    "        if nulls > 0:\n",
    "            df_ventas[col] = df_ventas[col].ffill().bfill()\n",
    "            imputation_stats[\"ventas\"][f\"{col}_filled\"] = int(nulls)\n",
    "\n",
    "# Unidades Total\n",
    "if \"total_unidades_entregadas\" in df_ventas.columns:\n",
    "    s_total = df_ventas[\"total_unidades_entregadas\"]\n",
    "    # Interpolación Lineal (cubre gaps pequeños y \"unir puntos\")\n",
    "    s_interp = s_total.interpolate(method='linear')\n",
    "    df_ventas[\"total_unidades_entregadas\"] = s_interp.fillna(0)\n",
    "\n",
    "# Desglose\n",
    "for col in [\"unidades_promo_pagadas\", \"unidades_promo_bonificadas\"]:\n",
    "    if col in df_ventas.columns:\n",
    "        df_ventas[col] = df_ventas[col].fillna(0)\n",
    "\n",
    "if \"unidades_precio_normal\" in df_ventas.columns:\n",
    "    # Residual\n",
    "    residual = df_ventas[\"total_unidades_entregadas\"] - (df_ventas[\"unidades_promo_pagadas\"] + df_ventas[\"unidades_promo_bonificadas\"])\n",
    "    df_ventas[\"unidades_precio_normal\"] = df_ventas[\"unidades_precio_normal\"].fillna(residual)\n",
    "    # Clip a 0\n",
    "    df_ventas[\"unidades_precio_normal\"] = df_ventas[\"unidades_precio_normal\"].clip(lower=0)\n",
    "\n",
    "print(\"Imputación de Negocio completada.\")\n",
    "print(\"Stats Imputación:\", imputation_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59f6c8ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.845449Z",
     "iopub.status.busy": "2026-02-15T17:01:31.844693Z",
     "iopub.status.idle": "2026-02-15T17:01:31.851837Z",
     "shell.execute_reply": "2026-02-15T17:01:31.851277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recálculo Financiero Selectivo...\n",
      "No hay filas imputadas para recalcular.\n"
     ]
    }
   ],
   "source": [
    "# Celda 10: Recálculo Financiero Selectivo\n",
    "print(\"Recálculo Financiero Selectivo...\")\n",
    "recalc_flag = config[\"preprocessing\"].get(\"recalc_financials\", False)\n",
    "\n",
    "if recalc_flag:\n",
    "    # Solo filas imputadas en ventas\n",
    "    if imputed_sales_mask.any():\n",
    "        count = imputed_sales_mask.sum()\n",
    "        print(f\"Recalculando {count} filas imputadas...\")\n",
    "        \n",
    "        # Indices\n",
    "        idx = df_ventas[imputed_sales_mask].index\n",
    "        \n",
    "        # Costo Total\n",
    "        df_ventas.loc[idx, \"costo_total\"] = (\n",
    "            df_ventas.loc[idx, \"total_unidades_entregadas\"] * \n",
    "            df_ventas.loc[idx, \"costo_unitario\"]\n",
    "        )\n",
    "        \n",
    "        # Ingresos Totales (Normal + Promo Pagada) * Precio\n",
    "        unidades_pagas = (\n",
    "            df_ventas.loc[idx, \"unidades_precio_normal\"] + \n",
    "            df_ventas.loc[idx, \"unidades_promo_pagadas\"]\n",
    "        )\n",
    "        # FIX: Usar precio_unitario_full\n",
    "        df_ventas.loc[idx, \"ingresos_totales\"] = (\n",
    "            unidades_pagas * df_ventas.loc[idx, \"precio_unitario_full\"]\n",
    "        )\n",
    "        \n",
    "        # Utilidad\n",
    "        df_ventas.loc[idx, \"utilidad\"] = (\n",
    "            df_ventas.loc[idx, \"ingresos_totales\"] - \n",
    "            df_ventas.loc[idx, \"costo_total\"]\n",
    "        )\n",
    "    else:\n",
    "        print(\"No hay filas imputadas para recalcular.\")\n",
    "else:\n",
    "    print(\"Recálculo financiero desactivado en config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4dace80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.855373Z",
     "iopub.status.busy": "2026-02-15T17:01:31.854995Z",
     "iopub.status.idle": "2026-02-15T17:01:31.872362Z",
     "shell.execute_reply": "2026-02-15T17:01:31.872362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregación Mensual (MS)...\n",
      "Agregación completada.\n"
     ]
    }
   ],
   "source": [
    "# Celda 11: Agregación Mensual\n",
    "print(\"Agregación Mensual (MS)...\")\n",
    "agg_rules = config[\"preprocessing\"].get(\"aggregation_rules\", {})\n",
    "\n",
    "monthly_dfs = {}\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    if \"fecha\" in df.columns:\n",
    "        df = df.set_index(\"fecha\")\n",
    "    \n",
    "    # Filtrar reglas para este DF\n",
    "    current_rules = {col: agg_rules[col] for col in df.columns if col in agg_rules}\n",
    "    \n",
    "    # Reglas especiales\n",
    "    if key == \"promo\" and \"es_promo\" in df.columns:\n",
    "        current_rules[\"es_promo\"] = \"sum\" # Cuenta días\n",
    "    elif key == \"macro\":\n",
    "        current_rules = {col: \"first\" for col in df.columns}\n",
    "    \n",
    "    # Resample\n",
    "    if current_rules:\n",
    "        df_monthly = df.resample(\"MS\").agg(current_rules)\n",
    "    else:\n",
    "        df_monthly = df.resample(\"MS\").sum(numeric_only=True)\n",
    "    \n",
    "    # Renombres post-agregación\n",
    "    if key == \"promo\" and \"es_promo\" in df_monthly.columns:\n",
    "        df_monthly.rename(columns={\"es_promo\": \"dias_en_promo\"}, inplace=True)\n",
    "        \n",
    "    monthly_dfs[key] = df_monthly\n",
    "\n",
    "print(\"Agregación completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da9ee4b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.875557Z",
     "iopub.status.busy": "2026-02-15T17:01:31.875557Z",
     "iopub.status.idle": "2026-02-15T17:01:31.883149Z",
     "shell.execute_reply": "2026-02-15T17:01:31.882401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unificando Datasets...\n",
      "Dataset Maestro Mensual: (98, 18)\n"
     ]
    }
   ],
   "source": [
    "# Celda 12: Unificación de Fuentes (Merging)\n",
    "print(\"Unificando Datasets...\")\n",
    "df_master = monthly_dfs[\"ventas\"].copy()\n",
    "\n",
    "for key in [\"marketing\", \"promo\", \"macro\"]:\n",
    "    other_df = monthly_dfs[key]\n",
    "    # Merge por índice (fechas mes)\n",
    "    df_master = df_master.merge(other_df, left_index=True, right_index=True, how=\"left\")\n",
    "\n",
    "print(f\"Dataset Maestro Mensual: {df_master.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdbec1fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.884332Z",
     "iopub.status.busy": "2026-02-15T17:01:31.884332Z",
     "iopub.status.idle": "2026-02-15T17:01:31.892092Z",
     "shell.execute_reply": "2026-02-15T17:01:31.891445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputación Final (Huecos Estructurales)...\n",
      "Dataset limpio.\n"
     ]
    }
   ],
   "source": [
    "# Celda 13: Imputación Post-Merge\n",
    "print(\"Imputación Final (Huecos Estructurales)...\")\n",
    "df_master = df_master.interpolate(method='linear').ffill().bfill()\n",
    "\n",
    "nulos = df_master.isna().sum().sum()\n",
    "if nulos > 0:\n",
    "    print(f\"ADVERTENCIA: Quedan {nulos} valores nulos.\")\n",
    "else:\n",
    "    print(\"Dataset limpio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b42af80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.894919Z",
     "iopub.status.busy": "2026-02-15T17:01:31.894380Z",
     "iopub.status.idle": "2026-02-15T17:01:31.900360Z",
     "shell.execute_reply": "2026-02-15T17:01:31.899490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando Regla de Oro: Eliminación del mes en curso (incompleto)...\n",
      "  - Detectado mes incompleto o en curso: 2026-02. Eliminando para evitar Data Leakage.\n",
      "Dataset Maestro Final (Meses Cerrados): (97, 18)\n"
     ]
    }
   ],
   "source": [
    "# Celda 14: Regla de Oro (Anti-Data Leakage)\n",
    "print(\"Aplicando Regla de Oro: Eliminación del mes en curso (incompleto)...\")\n",
    "current_date = datetime.now()\n",
    "if not df_master.empty:\n",
    "    last_date = df_master.index.max()\n",
    "    # Verificar si el último mes del dataset coincide con el mes/año actual\n",
    "    if last_date.year == current_date.year and last_date.month == current_date.month:\n",
    "        print(f\"  - Detectado mes incompleto o en curso: {last_date.strftime('%Y-%m')}. Eliminando para evitar Data Leakage.\")\n",
    "        df_master = df_master.iloc[:-1]\n",
    "    else:\n",
    "        print(f\"  - El último mes ({last_date.strftime('%Y-%m')}) es anterior al actual. No se requiere corte.\")\n",
    "\n",
    "print(f\"Dataset Maestro Final (Meses Cerrados): {df_master.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6a40222",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.902489Z",
     "iopub.status.busy": "2026-02-15T17:01:31.902489Z",
     "iopub.status.idle": "2026-02-15T17:01:31.911919Z",
     "shell.execute_reply": "2026-02-15T17:01:31.911919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado exitoso en: C:\\Users\\USUARIO\\Documents\\Forecaster\\Forecaster_MisBunuelos\\data\\02_cleansed\\master_monthly.parquet\n"
     ]
    }
   ],
   "source": [
    "# Celda 15: Exportación\n",
    "output_file = CLEANSED_DATA_PATH / \"master_monthly.parquet\"\n",
    "df_master.to_parquet(output_file)\n",
    "print(f\"Guardado exitoso en: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98f4ad31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T17:01:31.914700Z",
     "iopub.status.busy": "2026-02-15T17:01:31.914700Z",
     "iopub.status.idle": "2026-02-15T17:01:31.935454Z",
     "shell.execute_reply": "2026-02-15T17:01:31.934915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte DETALLADO generado en: C:\\Users\\USUARIO\\Documents\\Forecaster\\Forecaster_MisBunuelos\\experiments\\phase_02_preprocessing\\artifacts\\phase_02_preprocessing.json\n",
      "Resumen del Artefacto de Salida:\n",
      "{\n",
      "  \"file_name\": \"master_monthly.parquet\",\n",
      "  \"full_path\": \"C:\\\\Users\\\\USUARIO\\\\Documents\\\\Forecaster\\\\Forecaster_MisBunuelos\\\\data\\\\02_cleansed\\\\master_monthly.parquet\",\n",
      "  \"file_size_bytes\": 20665,\n",
      "  \"shape\": {\n",
      "    \"rows\": 97,\n",
      "    \"columns\": 18\n",
      "  },\n",
      "  \"temporal_coverage\": {\n",
      "    \"start_date\": \"2018-01-01T00:00:00\",\n",
      "    \"end_date\": \"2026-01-01T00:00:00\",\n",
      "    \"frequency\": \"MS (Month Start)\",\n",
      "    \"total_months\": 97,\n",
      "    \"is_series_complete\": true,\n",
      "    \"missing_expected_dates\": [],\n",
      "    \"duplicate_dates_count\": 0\n",
      "  },\n",
      "  \"data_integrity\": {\n",
      "    \"duplicate_rows\": 0,\n",
      "    \"rows_with_nulls\": 0,\n",
      "    \"total_nulls\": 0\n",
      "  },\n",
      "  \"schema\": {\n",
      "    \"columns\": [\n",
      "      \"total_unidades_entregadas\",\n",
      "      \"unidades_precio_normal\",\n",
      "      \"unidades_promo_pagadas\",\n",
      "      \"unidades_promo_bonificadas\",\n",
      "      \"precio_unitario_full\",\n",
      "      \"costo_unitario\",\n",
      "      \"ingresos_totales\",\n",
      "      \"costo_total\",\n",
      "      \"utilidad\",\n",
      "      \"inversion_facebook\",\n",
      "      \"inversion_instagram\",\n",
      "      \"inversion_total\",\n",
      "      \"dias_en_promo\",\n",
      "      \"ipc_mensual\",\n",
      "      \"trm_promedio\",\n",
      "      \"tasa_desempleo\",\n",
      "      \"costo_insumos_index\",\n",
      "      \"confianza_consumidor\"\n",
      "    ],\n",
      "    \"dtypes\": {\n",
      "      \"total_unidades_entregadas\": \"int64\",\n",
      "      \"unidades_precio_normal\": \"int64\",\n",
      "      \"unidades_promo_pagadas\": \"int64\",\n",
      "      \"unidades_promo_bonificadas\": \"int64\",\n",
      "      \"precio_unitario_full\": \"float64\",\n",
      "      \"costo_unitario\": \"float64\",\n",
      "      \"ingresos_totales\": \"int64\",\n",
      "      \"costo_total\": \"float64\",\n",
      "      \"utilidad\": \"float64\",\n",
      "      \"inversion_facebook\": \"float64\",\n",
      "      \"inversion_instagram\": \"float64\",\n",
      "      \"inversion_total\": \"float64\",\n",
      "      \"dias_en_promo\": \"int64\",\n",
      "      \"ipc_mensual\": \"float64\",\n",
      "      \"trm_promedio\": \"float64\",\n",
      "      \"tasa_desempleo\": \"float64\",\n",
      "      \"costo_insumos_index\": \"float64\",\n",
      "      \"confianza_consumidor\": \"float64\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Celda 16: Reporte JSON Detallado (Enhanced + Integrity Checks)\n",
    "import json\n",
    "import platform\n",
    "\n",
    "# Recopilar métricas finales del artefacto de salida\n",
    "final_shape = df_master.shape\n",
    "\n",
    "# 1. Validaciones Temporales\n",
    "is_series_complete = False\n",
    "missing_expected_dates = []\n",
    "duplicate_dates_count = 0\n",
    "date_min = \"N/A\"\n",
    "date_max = \"N/A\"\n",
    "total_months = 0\n",
    "\n",
    "if isinstance(df_master.index, pd.DatetimeIndex):\n",
    "    if not df_master.empty:\n",
    "        date_min = df_master.index.min().isoformat()\n",
    "        date_max = df_master.index.max().isoformat()\n",
    "        total_months = len(df_master)\n",
    "        \n",
    "        # Chequear completitud (Freq MS)\n",
    "        expected_range = pd.date_range(start=df_master.index.min(), end=df_master.index.max(), freq='MS')\n",
    "        # Comparar sets para ignorar duplicados en index por ahora (se revisan abajo)\n",
    "        is_series_complete = len(expected_range) == len(set(df_master.index))\n",
    "        if not is_series_complete:\n",
    "             missing_expected_dates = [d.isoformat() for d in set(expected_range) - set(df_master.index)]\n",
    "        \n",
    "        # Chequear fechas duplicadas\n",
    "        duplicate_dates_count = int(df_master.index.duplicated().sum())\n",
    "\n",
    "# 2. Integridad de Datos\n",
    "duplicate_rows = int(df_master.duplicated().sum())\n",
    "total_nulls = int(df_master.isna().sum().sum())\n",
    "rows_with_nulls = int(df_master.isna().any(axis=1).sum())\n",
    "column_types = df_master.dtypes.astype(str).to_dict()\n",
    "columns_list = df_master.columns.tolist()\n",
    "\n",
    "file_size_bytes = output_file.stat().st_size if output_file.exists() else 0\n",
    "\n",
    "missing_values = total_nulls\n",
    "\n",
    "# 3. Muestras de Datos (Serialize dates/timestamps for JSON)\n",
    "def serialize_df(df_part):\n",
    "    # Convert timestamp index to string column for JSON\n",
    "    temp = df_part.copy()\n",
    "    if isinstance(temp.index, pd.DatetimeIndex):\n",
    "        temp = temp.reset_index()\n",
    "        temp['fecha'] = temp['fecha'].astype(str)\n",
    "    return temp.to_dict(orient='records')\n",
    "\n",
    "head_5 = serialize_df(df_master.head(5))\n",
    "tail_5 = serialize_df(df_master.tail(5))\n",
    "random_5 = serialize_df(df_master.sample(5, random_state=42))\n",
    "\n",
    "# Estructura enriquecida del reporte\n",
    "report = {\n",
    "    \"phase\": \"Phase 2 - Preprocessing\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"environment_info\": {\n",
    "        \"platform\": platform.system(),\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"pandas_version\": pd.__version__\n",
    "    },\n",
    "    \"execution_context\": {\n",
    "        \"description\": \"Limpieza exhaustiva, imputación de negocio, agregación mensual y corte de mes en curso (Anti-Data Leakage).\",\n",
    "        \"validation_status\": \"SUCCESS\" if total_nulls == 0 and is_series_complete and duplicate_dates_count == 0 else \"WARNING\"\n",
    "    },\n",
    "    \"data_quality_audit\": {\n",
    "        \"contract_validation\": data_contract_status if 'data_contract_status' in locals() else {},\n",
    "        \"schema_enforcement\": {\n",
    "            \"columns_removed\": columns_removed_log if 'columns_removed_log' in locals() else {}\n",
    "        },\n",
    "        \"cleaning_stats\": {\n",
    "            \"rows_filtered_logic\": stats_cleaning.get(\"filtered\", {}),\n",
    "            \"duplicates_removed\": stats_cleaning.get(\"duplicates\", {}),\n",
    "            \"sentinel_values_replaced\": sentinel_stats,\n",
    "            \"temporal_gaps_reindexed\": reindex_stats\n",
    "        },\n",
    "        \"imputation_metrics\": {\n",
    "            \"financial_records_recalculated\": int(imputed_sales_mask.sum()) if 'imputed_sales_mask' in locals() else 0,\n",
    "            \"remaining_nulls_final\": total_nulls,\n",
    "            \"details\": imputation_stats if 'imputation_stats' in locals() else {}\n",
    "        }\n",
    "    },\n",
    "    \"output_artifact_details\": {\n",
    "        \"file_name\": output_file.name,\n",
    "        \"full_path\": str(output_file.absolute()),\n",
    "        \"file_size_bytes\": file_size_bytes,\n",
    "        \"shape\": {\n",
    "            \"rows\": final_shape[0],\n",
    "            \"columns\": final_shape[1]\n",
    "        },\n",
    "        \"temporal_coverage\": {\n",
    "            \"start_date\": date_min,\n",
    "            \"end_date\": date_max,\n",
    "            \"frequency\": \"MS (Month Start)\",\n",
    "            \"total_months\": total_months,\n",
    "            \"is_series_complete\": is_series_complete,\n",
    "            \"missing_expected_dates\": missing_expected_dates,\n",
    "            \"duplicate_dates_count\": duplicate_dates_count\n",
    "        },\n",
    "        \"data_integrity\": {\n",
    "            \"duplicate_rows\": duplicate_rows,\n",
    "            \"rows_with_nulls\": rows_with_nulls,\n",
    "            \"total_nulls\": total_nulls\n",
    "        },\n",
    "        \"schema\": {\n",
    "            \"columns\": columns_list,\n",
    "            \"dtypes\": column_types\n",
    "        }\n",
    "    },\n",
    "    \"sample_data\": {\n",
    "        \"head_5\": head_5,\n",
    "        \"tail_5\": tail_5,\n",
    "        \"random_5\": random_5\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar reporte\n",
    "report_path = ARTIFACTS_PATH / \"phase_02_preprocessing.json\"\n",
    "with open(report_path, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(report, f, indent=4)\n",
    "\n",
    "print(f\"Reporte DETALLADO generado en: {report_path}\")\n",
    "print(\"Resumen del Artefacto de Salida:\")\n",
    "print(json.dumps(report[\"output_artifact_details\"], indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
