{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 1: Data Discovery (Incremental Extraction)\n",
                "\n",
                "This notebook handles the extraction of data from Supabase. It implements an incremental load strategy to minimize data transfer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Project Root detected at: c:\\Users\\USUARIO\\Documents\\Forecaster\\Forecaster_MisBunuelos\n",
                        "Working directory set to: c:\\Users\\USUARIO\\Documents\\Forecaster\\Forecaster_MisBunuelos\n",
                        "Supabase client initialized successfully.\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import yaml\n",
                "import pathlib\n",
                "import datetime\n",
                "import json\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# --- ROBUST PATH CONFIGURATION ---\n",
                "# Identify project root explicitly to handle running from 'notebooks/' or root\n",
                "current_dir = pathlib.Path.cwd()\n",
                "\n",
                "if current_dir.name == \"notebooks\":\n",
                "    project_root = current_dir.parent\n",
                "elif (current_dir / \"notebooks\").exists():\n",
                "    project_root = current_dir\n",
                "else:\n",
                "    # Fallback: traverse up until markers found or root reached\n",
                "    project_root = current_dir\n",
                "    for parent in current_dir.parents:\n",
                "        if (parent / \"config.yaml\").exists():\n",
                "            project_root = parent\n",
                "            break\n",
                "\n",
                "print(f\"Project Root detected at: {project_root}\")\n",
                "\n",
                "# Add root to sys.path for src imports\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.insert(0, str(project_root))\n",
                "\n",
                "# Set working directory to root for relative file access (config.yaml, .env)\n",
                "os.chdir(project_root)\n",
                "print(f\"Working directory set to: {os.getcwd()}\")\n",
                "\n",
                "# --- IMPORTS ---\n",
                "from src.connectors.supabase_connector import get_supabase_client\n",
                "\n",
                "# Load Configuration\n",
                "try:\n",
                "    with open(\"config.yaml\", \"r\") as f:\n",
                "        config = yaml.safe_load(f)\n",
                "except FileNotFoundError:\n",
                "    print(\"Error: config.yaml not found in project root.\")\n",
                "    raise\n",
                "\n",
                "# Define Paths\n",
                "RAW_DATA_PATH = pathlib.Path(config['paths']['data']['raw'])\n",
                "RAW_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "EXPERIMENT_ARTIFACTS_PATH = pathlib.Path(\"experiments\") / \"phase_01_discovery\" / \"artifacts\"\n",
                "EXPERIMENT_ARTIFACTS_PATH.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "FIGURES_PATH = pathlib.Path(\"experiments\") / \"phase_01_discovery\" / \"figures\"\n",
                "FIGURES_PATH.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Initialize Supabase Client\n",
                "try:\n",
                "    supabase = get_supabase_client()\n",
                "    print(\"Supabase client initialized successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"Failed to initialize Supabase client: {e}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_remote_max_date(table_name, date_col):\n",
                "    \"\"\"\n",
                "    Queries Supabase for the maximum date in the given table.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        response = supabase.table(table_name).select(date_col).order(date_col, desc=True).limit(1).execute()\n",
                "        data = response.data\n",
                "        if data:\n",
                "            return data[0][date_col]\n",
                "        return None\n",
                "    except Exception as e:\n",
                "        print(f\"Error getting max date for {table_name}: {e}\")\n",
                "        return None\n",
                "\n",
                "def download_data(table_name, date_col, greater_than=None):\n",
                "    \"\"\"\n",
                "    Downloads data from Supabase. With manual pagination loop.\n",
                "    \"\"\"\n",
                "    query = supabase.table(table_name).select(\"*\")\n",
                "    if greater_than:\n",
                "        query = query.gt(date_col, greater_than)\n",
                "    \n",
                "    query = query.order(date_col) # Ensure stable ordering\n",
                "    \n",
                "    all_rows = []\n",
                "    start = 0\n",
                "    batch_size = 1000\n",
                "    while True:\n",
                "        try:\n",
                "             r = query.range(start, start + batch_size - 1).execute()\n",
                "             rows = r.data\n",
                "             if not rows:\n",
                "                 break\n",
                "             all_rows.extend(rows)\n",
                "             if len(rows) < batch_size:\n",
                "                 break\n",
                "             start += batch_size\n",
                "        except Exception as e:\n",
                "            print(f\"Error downloading {table_name}: {e}\")\n",
                "            break\n",
                "            \n",
                "    return pd.DataFrame(all_rows)\n",
                "\n",
                "def sync_table(table_name, date_col, full_update):\n",
                "    local_file = RAW_DATA_PATH / f\"{table_name}.parquet\"\n",
                "    operation_status = \"Up to Date\"\n",
                "    new_rows_count = 0\n",
                "    \n",
                "    # Check local state\n",
                "    local_df = pd.DataFrame()\n",
                "    max_local = None\n",
                "    \n",
                "    if local_file.exists() and not full_update:\n",
                "        try:\n",
                "            local_df = pd.read_parquet(local_file)\n",
                "            if not local_df.empty and date_col in local_df.columns:\n",
                "                max_local = local_df[date_col].max()\n",
                "                # Handle timestamp types for comparison\n",
                "                if isinstance(max_local, (pd.Timestamp, datetime.date, datetime.datetime)):\n",
                "                    max_local = max_local.strftime('%Y-%m-%d')\n",
                "        except Exception as e:\n",
                "            print(f\"Error reading local file {local_file}: {e}. Will trigger full update.\")\n",
                "            max_local = None\n",
                "    \n",
                "    # Decide action\n",
                "    if not local_file.exists() or full_update or max_local is None:\n",
                "        print(f\"Downloading FULL table: {table_name}...\")\n",
                "        new_df = download_data(table_name, date_col)\n",
                "        if not new_df.empty:\n",
                "            if date_col in new_df.columns:\n",
                "                new_df[date_col] = pd.to_datetime(new_df[date_col])\n",
                "            # Ensure consistency\n",
                "            new_df.sort_values(by=date_col, inplace=True)\n",
                "            new_df.to_parquet(local_file, index=False)\n",
                "            operation_status = \"Full Download\"\n",
                "            new_rows_count = len(new_df)\n",
                "            final_df = new_df\n",
                "        else:\n",
                "            final_df = pd.DataFrame()\n",
                "            print(f\"Warning: No data found for {table_name}\")\n",
                "    else:\n",
                "        # Check remote max\n",
                "        max_remote = get_remote_max_date(table_name, date_col)\n",
                "        \n",
                "        # Compare\n",
                "        trigger_update = False\n",
                "        if max_remote:\n",
                "             # Simple string comparison usually works for ISO dates\n",
                "             if str(max_remote) > str(max_local):\n",
                "                 trigger_update = True\n",
                "        \n",
                "        if trigger_update:\n",
                "             print(f\"Downloading INCREMENTAL {table_name} (Remote {max_remote} > Local {max_local}) ...\")\n",
                "             delta_df = download_data(table_name, date_col, greater_than=max_local)\n",
                "             if not delta_df.empty:\n",
                "                 if date_col in delta_df.columns:\n",
                "                     delta_df[date_col] = pd.to_datetime(delta_df[date_col])\n",
                "                 \n",
                "                 # Concat and dedup\n",
                "                 final_df = pd.concat([local_df, delta_df])\n",
                "                 final_df.drop_duplicates(subset=[date_col], keep='last', inplace=True)\n",
                "                 final_df.sort_values(by=date_col, inplace=True)\n",
                "                 \n",
                "                 final_df.to_parquet(local_file, index=False)\n",
                "                 \n",
                "                 operation_status = \"Incremental Download\"\n",
                "                 new_rows_count = len(delta_df)\n",
                "             else:\n",
                "                 final_df = local_df\n",
                "        else:\n",
                "             print(f\"Table {table_name} is up to date. (Max: {max_local})\")\n",
                "             final_df = local_df\n",
                "\n",
                "    total_rows = len(final_df)\n",
                "    \n",
                "    return {\n",
                "        \"table\": table_name,\n",
                "        \"status\": operation_status,\n",
                "        \"new_rows\": new_rows_count,\n",
                "        \"total_rows\": total_rows,\n",
                "        \"download_timestamp\": datetime.datetime.now().isoformat()\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting pipeline. Tables: ['ventas_diarias', 'redes_sociales', 'promocion_diaria', 'macro_economia'], Full Update: False\n",
                        "Table ventas_diarias is up to date. (Max: 2026-02-10)\n",
                        "Table ventas_diarias: [Up to Date] (New: 0, Total: 2963)\n",
                        "Table redes_sociales is up to date. (Max: 2026-02-10)\n",
                        "Table redes_sociales: [Up to Date] (New: 0, Total: 2963)\n",
                        "Table promocion_diaria is up to date. (Max: 2026-02-10)\n",
                        "Table promocion_diaria: [Up to Date] (New: 0, Total: 2963)\n",
                        "Table macro_economia is up to date. (Max: 2026-02-01)\n",
                        "Table macro_economia: [Up to Date] (New: 0, Total: 98)\n"
                    ]
                }
            ],
            "source": [
                "download_details = []\n",
                "source_tables = config['data']['source_tables']\n",
                "full_update_flag = config['data']['full_update']\n",
                "date_column = config['data']['date_column']\n",
                "\n",
                "print(f\"Starting pipeline. Tables: {source_tables}, Full Update: {full_update_flag}\")\n",
                "\n",
                "for table in source_tables:\n",
                "    result = sync_table(table, date_column, full_update_flag)\n",
                "    download_details.append(result)\n",
                "    print(f\"Table {table}: [{result['status']}] (New: {result['new_rows']}, Total: {result['total_rows']})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- SANITY CHECK & VALIDATION ---\n",
                        "\n",
                        "--- ventas_diarias ---\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 2963 entries, 0 to 2962\n",
                        "Data columns (total 11 columns):\n",
                        " #   Column                      Non-Null Count  Dtype         \n",
                        "---  ------                      --------------  -----         \n",
                        " 0   id                          2963 non-null   int64         \n",
                        " 1   fecha                       2963 non-null   datetime64[ns]\n",
                        " 2   total_unidades_entregadas   2963 non-null   int64         \n",
                        " 3   unidades_precio_normal      2963 non-null   int64         \n",
                        " 4   unidades_promo_pagadas      2963 non-null   int64         \n",
                        " 5   unidades_promo_bonificadas  2963 non-null   int64         \n",
                        " 6   precio_unitario_full        2963 non-null   int64         \n",
                        " 7   costo_unitario              2963 non-null   int64         \n",
                        " 8   ingresos_totales            2963 non-null   int64         \n",
                        " 9   costo_total                 2963 non-null   float64       \n",
                        " 10  utilidad                    2963 non-null   float64       \n",
                        "dtypes: datetime64[ns](1), float64(2), int64(8)\n",
                        "memory usage: 254.8 KB\n",
                        "None\n",
                        "   id      fecha  total_unidades_entregadas  unidades_precio_normal  \\\n",
                        "0   1 2018-01-01                        366                     366   \n",
                        "1   2 2018-01-02                        389                     389   \n",
                        "2   3 2018-01-03                        389                     389   \n",
                        "3   4 2018-01-04                        389                     389   \n",
                        "4   5 2018-01-05                        497                     497   \n",
                        "\n",
                        "   unidades_promo_pagadas  unidades_promo_bonificadas  precio_unitario_full  \\\n",
                        "0                       0                           0                   600   \n",
                        "1                       0                           0                   600   \n",
                        "2                       0                           0                   600   \n",
                        "3                       0                           0                   600   \n",
                        "4                       0                           0                   600   \n",
                        "\n",
                        "   costo_unitario  ingresos_totales  costo_total  utilidad  \n",
                        "0             200            219600      73200.0  146400.0  \n",
                        "1             200            233400      77800.0  155600.0  \n",
                        "2             200            233400      77800.0  155600.0  \n",
                        "3             200            233400      77800.0  155600.0  \n",
                        "4             200            298200      99400.0  198800.0  \n",
                        "\n",
                        "\n",
                        "SUCCESS: ventas_diarias history sufficient (98.7 months)\n",
                        "--- redes_sociales ---\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 2963 entries, 0 to 2962\n",
                        "Data columns (total 6 columns):\n",
                        " #   Column                  Non-Null Count  Dtype         \n",
                        "---  ------                  --------------  -----         \n",
                        " 0   id                      2963 non-null   int64         \n",
                        " 1   fecha                   2963 non-null   datetime64[ns]\n",
                        " 2   campaña                 2963 non-null   object        \n",
                        " 3   inversion_facebook      2963 non-null   float64       \n",
                        " 4   inversion_instagram     2963 non-null   float64       \n",
                        " 5   inversion_total_diaria  2963 non-null   float64       \n",
                        "dtypes: datetime64[ns](1), float64(3), int64(1), object(1)\n",
                        "memory usage: 139.0+ KB\n",
                        "None\n",
                        "   id      fecha      campaña  inversion_facebook  inversion_instagram  \\\n",
                        "0   1 2018-01-01  Sin Campaña                 0.0                  0.0   \n",
                        "1   2 2018-01-02  Sin Campaña                 0.0                  0.0   \n",
                        "2   3 2018-01-03  Sin Campaña                 0.0                  0.0   \n",
                        "3   4 2018-01-04  Sin Campaña                 0.0                  0.0   \n",
                        "4   5 2018-01-05  Sin Campaña                 0.0                  0.0   \n",
                        "\n",
                        "   inversion_total_diaria  \n",
                        "0                     0.0  \n",
                        "1                     0.0  \n",
                        "2                     0.0  \n",
                        "3                     0.0  \n",
                        "4                     0.0  \n",
                        "\n",
                        "\n",
                        "--- promocion_diaria ---\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 2963 entries, 0 to 2962\n",
                        "Data columns (total 3 columns):\n",
                        " #   Column    Non-Null Count  Dtype         \n",
                        "---  ------    --------------  -----         \n",
                        " 0   id        2963 non-null   int64         \n",
                        " 1   fecha     2963 non-null   datetime64[ns]\n",
                        " 2   es_promo  2963 non-null   int64         \n",
                        "dtypes: datetime64[ns](1), int64(2)\n",
                        "memory usage: 69.6 KB\n",
                        "None\n",
                        "   id      fecha  es_promo\n",
                        "0   1 2018-01-01         0\n",
                        "1   2 2018-01-02         0\n",
                        "2   3 2018-01-03         0\n",
                        "3   4 2018-01-04         0\n",
                        "4   5 2018-01-05         0\n",
                        "\n",
                        "\n",
                        "--- macro_economia ---\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 98 entries, 0 to 97\n",
                        "Data columns (total 7 columns):\n",
                        " #   Column                Non-Null Count  Dtype         \n",
                        "---  ------                --------------  -----         \n",
                        " 0   id                    98 non-null     int64         \n",
                        " 1   fecha                 98 non-null     datetime64[ns]\n",
                        " 2   ipc_mensual           98 non-null     float64       \n",
                        " 3   trm_promedio          98 non-null     float64       \n",
                        " 4   tasa_desempleo        98 non-null     float64       \n",
                        " 5   costo_insumos_index   98 non-null     float64       \n",
                        " 6   confianza_consumidor  98 non-null     float64       \n",
                        "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
                        "memory usage: 5.5 KB\n",
                        "None\n",
                        "   id      fecha  ipc_mensual  trm_promedio  tasa_desempleo  \\\n",
                        "0   1 2018-01-01       0.5097       2991.65           11.55   \n",
                        "1   2 2018-02-01       0.4811       2987.70           11.54   \n",
                        "2   3 2018-03-01       0.4955       3051.90           11.32   \n",
                        "3   4 2018-04-01       0.4405       3022.41           11.47   \n",
                        "4   5 2018-05-01       0.5689       3050.18           11.38   \n",
                        "\n",
                        "   costo_insumos_index  confianza_consumidor  \n",
                        "0                99.37                  4.26  \n",
                        "1               101.19                  3.73  \n",
                        "2               101.68                  7.30  \n",
                        "3               102.42                  9.29  \n",
                        "4               104.06                  6.98  \n",
                        "\n",
                        "\n",
                        "\n",
                        "Report saved to experiments\\phase_01_discovery\\artifacts\\phase_01_discovery.json\n"
                    ]
                }
            ],
            "source": [
                "validation_details = {}\n",
                "critical_check_passed = True\n",
                "\n",
                "print(\"\\n--- SANITY CHECK & VALIDATION ---\\n\")\n",
                "\n",
                "for table in source_tables:\n",
                "    file_path = RAW_DATA_PATH / f\"{table}.parquet\"\n",
                "    if file_path.exists():\n",
                "        df = pd.read_parquet(file_path)\n",
                "        print(f\"--- {table} ---\")\n",
                "        print(df.info())\n",
                "        print(df.head())\n",
                "        print(\"\\n\")\n",
                "        \n",
                "        # Validation checks\n",
                "        val_info = {\n",
                "            \"columns\": list(df.columns),\n",
                "            \"rows\": len(df),\n",
                "            \"completeness\": 1.0 # Placeholder\n",
                "        }\n",
                "        \n",
                "        # Critical Check: Ventas History > 36 Months\n",
                "        if table == \"ventas_diarias\":\n",
                "            if date_column in df.columns:\n",
                "                min_date = df[date_column].min()\n",
                "                max_date = df[date_column].max()\n",
                "                if pd.notnull(min_date) and pd.notnull(max_date):\n",
                "                    months_diff = (max_date - min_date) / pd.Timedelta(days=30)\n",
                "                    val_info[\"history_months\"] = round(months_diff, 2)\n",
                "                    \n",
                "                    if months_diff < 36:\n",
                "                        print(f\"CRITICAL WARNING: {table} has less than 36 months history ({months_diff:.1f})\")\n",
                "                        val_info[\"history_check\"] = \"FAIL\"\n",
                "                    else:\n",
                "                        print(f\"SUCCESS: {table} history sufficient ({months_diff:.1f} months)\")\n",
                "                        val_info[\"history_check\"] = \"PASS\"\n",
                "                else:\n",
                "                     val_info[\"history_check\"] = \"FAIL - Dates Null\"\n",
                "            else:\n",
                "                 val_info[\"history_check\"] = \"FAIL - No Date Col\"\n",
                "                 \n",
                "        validation_details[table] = val_info\n",
                "    else:\n",
                "        print(f\"WARNING: File for {table} not found.\")\n",
                "\n",
                "# JSON Report\n",
                "report = {\n",
                "    \"phase\": \"Phase 1 - Data Discovery\",\n",
                "    \"timestamp\": datetime.datetime.now().isoformat(),\n",
                "    \"description\": \"Incremental download and sanity check of raw data.\",\n",
                "    \"download_details\": download_details,\n",
                "    \"validation_details\": validation_details\n",
                "}\n",
                "\n",
                "report_path = EXPERIMENT_ARTIFACTS_PATH / \"phase_01_discovery.json\"\n",
                "with open(report_path, \"w\") as f:\n",
                "    json.dump(report, f, indent=4)\n",
                "\n",
                "print(f\"\\nReport saved to {report_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
