{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 1: Data Discovery (Incremental Extraction)\n",
                "\n",
                "This notebook handles the extraction of data from Supabase. It implements an incremental load strategy to minimize data transfer and performs comprehensive statistical analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Project Root detected at: c:\\Users\\USUARIO\\Documents\\Forecaster\\Forecaster_MisBunuelos\n",
                        "Working directory set to: c:\\Users\\USUARIO\\Documents\\Forecaster\\Forecaster_MisBunuelos\n",
                        "Supabase client initialized successfully.\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import yaml\n",
                "import pathlib\n",
                "import datetime\n",
                "import json\n",
                "import os\n",
                "import sys\n",
                "import numpy as np\n",
                "\n",
                "# --- ROBUST PATH CONFIGURATION ---\n",
                "# Identify project root explicitly to handle running from 'notebooks/' or root\n",
                "current_dir = pathlib.Path.cwd()\n",
                "\n",
                "if current_dir.name == \"notebooks\":\n",
                "    project_root = current_dir.parent\n",
                "elif (current_dir / \"notebooks\").exists():\n",
                "    project_root = current_dir\n",
                "else:\n",
                "    # Fallback: traverse up until markers found or root reached\n",
                "    project_root = current_dir\n",
                "    for parent in current_dir.parents:\n",
                "        if (parent / \"config.yaml\").exists():\n",
                "            project_root = parent\n",
                "            break\n",
                "\n",
                "print(f\"Project Root detected at: {project_root}\")\n",
                "\n",
                "# Add root to sys.path for src imports\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.insert(0, str(project_root))\n",
                "\n",
                "# Set working directory to root for relative file access (config.yaml, .env)\n",
                "os.chdir(project_root)\n",
                "print(f\"Working directory set to: {os.getcwd()}\")\n",
                "\n",
                "# --- IMPORTS ---\n",
                "from src.connectors.supabase_connector import get_supabase_client\n",
                "\n",
                "# Load Configuration\n",
                "try:\n",
                "    with open(\"config.yaml\", \"r\") as f:\n",
                "        config = yaml.safe_load(f)\n",
                "except FileNotFoundError:\n",
                "    print(\"Error: config.yaml not found in project root.\")\n",
                "    raise\n",
                "\n",
                "# Define Paths\n",
                "RAW_DATA_PATH = pathlib.Path(config['paths']['data']['raw'])\n",
                "RAW_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "EXPERIMENT_ARTIFACTS_PATH = pathlib.Path(\"experiments\") / \"phase_01_discovery\" / \"artifacts\"\n",
                "EXPERIMENT_ARTIFACTS_PATH.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "FIGURES_PATH = pathlib.Path(\"experiments\") / \"phase_01_discovery\" / \"figures\"\n",
                "FIGURES_PATH.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Initialize Supabase Client\n",
                "try:\n",
                "    supabase = get_supabase_client()\n",
                "    print(\"Supabase client initialized successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"Failed to initialize Supabase client: {e}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_remote_max_date(table_name, date_col):\n",
                "    \"\"\"\n",
                "    Queries Supabase for the maximum date in the given table.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        response = supabase.table(table_name).select(date_col).order(date_col, desc=True).limit(1).execute()\n",
                "        data = response.data\n",
                "        if data:\n",
                "            return data[0][date_col]\n",
                "        return None\n",
                "    except Exception as e:\n",
                "        print(f\"Error getting max date for {table_name}: {e}\")\n",
                "        return None\n",
                "\n",
                "def download_data(table_name, date_col, greater_than=None):\n",
                "    \"\"\"\n",
                "    Downloads data from Supabase. With manual pagination loop.\n",
                "    \"\"\"\n",
                "    query = supabase.table(table_name).select(\"*\")\n",
                "    if greater_than:\n",
                "        query = query.gt(date_col, greater_than)\n",
                "    \n",
                "    query = query.order(date_col) # Ensure stable ordering\n",
                "    \n",
                "    all_rows = []\n",
                "    start = 0\n",
                "    batch_size = 1000\n",
                "    while True:\n",
                "        try:\n",
                "             r = query.range(start, start + batch_size - 1).execute()\n",
                "             rows = r.data\n",
                "             if not rows:\n",
                "                 break\n",
                "             all_rows.extend(rows)\n",
                "             if len(rows) < batch_size:\n",
                "                 break\n",
                "             start += batch_size\n",
                "        except Exception as e:\n",
                "            print(f\"Error downloading {table_name}: {e}\")\n",
                "            break\n",
                "            \n",
                "    return pd.DataFrame(all_rows)\n",
                "\n",
                "def sync_table(table_name, date_col, full_update):\n",
                "    local_file = RAW_DATA_PATH / f\"{table_name}.parquet\"\n",
                "    operation_status = \"Up to Date\"\n",
                "    new_rows_count = 0\n",
                "    \n",
                "    # Check local state\n",
                "    local_df = pd.DataFrame()\n",
                "    max_local = None\n",
                "    \n",
                "    if local_file.exists() and not full_update:\n",
                "        try:\n",
                "            local_df = pd.read_parquet(local_file)\n",
                "            if not local_df.empty and date_col in local_df.columns:\n",
                "                max_local = local_df[date_col].max()\n",
                "                # Handle timestamp types for comparison\n",
                "                if isinstance(max_local, (pd.Timestamp, datetime.date, datetime.datetime)):\n",
                "                    max_local = max_local.strftime('%Y-%m-%d')\n",
                "        except Exception as e:\n",
                "            print(f\"Error reading local file {local_file}: {e}. Will trigger full update.\")\n",
                "            max_local = None\n",
                "    \n",
                "    # Decide action\n",
                "    if not local_file.exists() or full_update or max_local is None:\n",
                "        print(f\"Downloading FULL table: {table_name}...\")\n",
                "        new_df = download_data(table_name, date_col)\n",
                "        if not new_df.empty:\n",
                "            if date_col in new_df.columns:\n",
                "                new_df[date_col] = pd.to_datetime(new_df[date_col])\n",
                "            # Ensure consistency\n",
                "            new_df.sort_values(by=date_col, inplace=True)\n",
                "            new_df.to_parquet(local_file, index=False)\n",
                "            operation_status = \"Full Download\"\n",
                "            new_rows_count = len(new_df)\n",
                "            final_df = new_df\n",
                "        else:\n",
                "            final_df = pd.DataFrame()\n",
                "            print(f\"Warning: No data found for {table_name}\")\n",
                "    else:\n",
                "        # Check remote max\n",
                "        max_remote = get_remote_max_date(table_name, date_col)\n",
                "        \n",
                "        # Compare\n",
                "        trigger_update = False\n",
                "        if max_remote:\n",
                "             # Simple string comparison usually works for ISO dates\n",
                "             if str(max_remote) > str(max_local):\n",
                "                 trigger_update = True\n",
                "        \n",
                "        if trigger_update:\n",
                "             print(f\"Downloading INCREMENTAL {table_name} (Remote {max_remote} > Local {max_local}) ...\")\n",
                "             delta_df = download_data(table_name, date_col, greater_than=max_local)\n",
                "             if not delta_df.empty:\n",
                "                 if date_col in delta_df.columns:\n",
                "                     delta_df[date_col] = pd.to_datetime(delta_df[date_col])\n",
                "                 \n",
                "                 # Concat and dedup\n",
                "                 final_df = pd.concat([local_df, delta_df])\n",
                "                 final_df.drop_duplicates(subset=[date_col], keep='last', inplace=True)\n",
                "                 final_df.sort_values(by=date_col, inplace=True)\n",
                "                 \n",
                "                 final_df.to_parquet(local_file, index=False)\n",
                "                 \n",
                "                 operation_status = \"Incremental Download\"\n",
                "                 new_rows_count = len(delta_df)\n",
                "             else:\n",
                "                 final_df = local_df\n",
                "        else:\n",
                "             print(f\"Table {table_name} is up to date. (Max: {max_local})\")\n",
                "             final_df = local_df\n",
                "\n",
                "    total_rows = len(final_df)\n",
                "    \n",
                "    return {\n",
                "        \"table\": table_name,\n",
                "        \"status\": operation_status,\n",
                "        \"new_rows\": new_rows_count,\n",
                "        \"total_rows\": total_rows,\n",
                "        \"download_timestamp\": datetime.datetime.now().isoformat()\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting pipeline. Tables: ['ventas_diarias', 'redes_sociales', 'promocion_diaria', 'macro_economia'], Full Update: False\n",
                        "Table ventas_diarias is up to date. (Max: 2026-02-10)\n",
                        "Table ventas_diarias: [Up to Date] (New: 0, Total: 2963)\n",
                        "Table redes_sociales is up to date. (Max: 2026-02-10)\n",
                        "Table redes_sociales: [Up to Date] (New: 0, Total: 2963)\n",
                        "Table promocion_diaria is up to date. (Max: 2026-02-10)\n",
                        "Table promocion_diaria: [Up to Date] (New: 0, Total: 2963)\n",
                        "Table macro_economia is up to date. (Max: 2026-02-01)\n",
                        "Table macro_economia: [Up to Date] (New: 0, Total: 98)\n"
                    ]
                }
            ],
            "source": [
                "download_details = []\n",
                "source_tables = config['data']['source_tables']\n",
                "full_update_flag = config['data']['full_update']\n",
                "date_column = config['data']['date_column']\n",
                "\n",
                "print(f\"Starting pipeline. Tables: {source_tables}, Full Update: {full_update_flag}\")\n",
                "\n",
                "for table in source_tables:\n",
                "    result = sync_table(table, date_column, full_update_flag)\n",
                "    download_details.append(result)\n",
                "    print(f\"Table {table}: [{result['status']}] (New: {result['new_rows']}, Total: {result['total_rows']})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- SANITY CHECK ---\n",
                        "\n",
                        "\n",
                        "--- ventas_diarias ---\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 2963 entries, 0 to 2962\n",
                        "Data columns (total 11 columns):\n",
                        " #   Column                      Non-Null Count  Dtype         \n",
                        "---  ------                      --------------  -----         \n",
                        " 0   id                          2963 non-null   int64         \n",
                        " 1   fecha                       2963 non-null   datetime64[ns]\n",
                        " 2   total_unidades_entregadas   2963 non-null   int64         \n",
                        " 3   unidades_precio_normal      2963 non-null   int64         \n",
                        " 4   unidades_promo_pagadas      2963 non-null   int64         \n",
                        " 5   unidades_promo_bonificadas  2963 non-null   int64         \n",
                        " 6   precio_unitario_full        2963 non-null   int64         \n",
                        " 7   costo_unitario              2963 non-null   int64         \n",
                        " 8   ingresos_totales            2963 non-null   int64         \n",
                        " 9   costo_total                 2963 non-null   float64       \n",
                        " 10  utilidad                    2963 non-null   float64       \n",
                        "dtypes: datetime64[ns](1), float64(2), int64(8)\n",
                        "memory usage: 254.8 KB\n",
                        "None\n",
                        "   id      fecha  total_unidades_entregadas  unidades_precio_normal  \\\n",
                        "0   1 2018-01-01                        366                     366   \n",
                        "1   2 2018-01-02                        389                     389   \n",
                        "2   3 2018-01-03                        389                     389   \n",
                        "3   4 2018-01-04                        389                     389   \n",
                        "4   5 2018-01-05                        497                     497   \n",
                        "\n",
                        "   unidades_promo_pagadas  unidades_promo_bonificadas  precio_unitario_full  \\\n",
                        "0                       0                           0                   600   \n",
                        "1                       0                           0                   600   \n",
                        "2                       0                           0                   600   \n",
                        "3                       0                           0                   600   \n",
                        "4                       0                           0                   600   \n",
                        "\n",
                        "   costo_unitario  ingresos_totales  costo_total  utilidad  \n",
                        "0             200            219600      73200.0  146400.0  \n",
                        "1             200            233400      77800.0  155600.0  \n",
                        "2             200            233400      77800.0  155600.0  \n",
                        "3             200            233400      77800.0  155600.0  \n",
                        "4             200            298200      99400.0  198800.0  \n",
                        "SUCCESS: ventas_diarias history sufficient (98.7 months)\n",
                        "\n",
                        "--- redes_sociales ---\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 2963 entries, 0 to 2962\n",
                        "Data columns (total 6 columns):\n",
                        " #   Column                  Non-Null Count  Dtype         \n",
                        "---  ------                  --------------  -----         \n",
                        " 0   id                      2963 non-null   int64         \n",
                        " 1   fecha                   2963 non-null   datetime64[ns]\n",
                        " 2   campaña                 2963 non-null   object        \n",
                        " 3   inversion_facebook      2963 non-null   float64       \n",
                        " 4   inversion_instagram     2963 non-null   float64       \n",
                        " 5   inversion_total_diaria  2963 non-null   float64       \n",
                        "dtypes: datetime64[ns](1), float64(3), int64(1), object(1)\n",
                        "memory usage: 139.0+ KB\n",
                        "None\n",
                        "   id      fecha      campaña  inversion_facebook  inversion_instagram  \\\n",
                        "0   1 2018-01-01  Sin Campaña                 0.0                  0.0   \n",
                        "1   2 2018-01-02  Sin Campaña                 0.0                  0.0   \n",
                        "2   3 2018-01-03  Sin Campaña                 0.0                  0.0   \n",
                        "3   4 2018-01-04  Sin Campaña                 0.0                  0.0   \n",
                        "4   5 2018-01-05  Sin Campaña                 0.0                  0.0   \n",
                        "\n",
                        "   inversion_total_diaria  \n",
                        "0                     0.0  \n",
                        "1                     0.0  \n",
                        "2                     0.0  \n",
                        "3                     0.0  \n",
                        "4                     0.0  \n",
                        "\n",
                        "--- promocion_diaria ---\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 2963 entries, 0 to 2962\n",
                        "Data columns (total 3 columns):\n",
                        " #   Column    Non-Null Count  Dtype         \n",
                        "---  ------    --------------  -----         \n",
                        " 0   id        2963 non-null   int64         \n",
                        " 1   fecha     2963 non-null   datetime64[ns]\n",
                        " 2   es_promo  2963 non-null   int64         \n",
                        "dtypes: datetime64[ns](1), int64(2)\n",
                        "memory usage: 69.6 KB\n",
                        "None\n",
                        "   id      fecha  es_promo\n",
                        "0   1 2018-01-01         0\n",
                        "1   2 2018-01-02         0\n",
                        "2   3 2018-01-03         0\n",
                        "3   4 2018-01-04         0\n",
                        "4   5 2018-01-05         0\n",
                        "\n",
                        "--- macro_economia ---\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 98 entries, 0 to 97\n",
                        "Data columns (total 7 columns):\n",
                        " #   Column                Non-Null Count  Dtype         \n",
                        "---  ------                --------------  -----         \n",
                        " 0   id                    98 non-null     int64         \n",
                        " 1   fecha                 98 non-null     datetime64[ns]\n",
                        " 2   ipc_mensual           98 non-null     float64       \n",
                        " 3   trm_promedio          98 non-null     float64       \n",
                        " 4   tasa_desempleo        98 non-null     float64       \n",
                        " 5   costo_insumos_index   98 non-null     float64       \n",
                        " 6   confianza_consumidor  98 non-null     float64       \n",
                        "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
                        "memory usage: 5.5 KB\n",
                        "None\n",
                        "   id      fecha  ipc_mensual  trm_promedio  tasa_desempleo  \\\n",
                        "0   1 2018-01-01       0.5097       2991.65           11.55   \n",
                        "1   2 2018-02-01       0.4811       2987.70           11.54   \n",
                        "2   3 2018-03-01       0.4955       3051.90           11.32   \n",
                        "3   4 2018-04-01       0.4405       3022.41           11.47   \n",
                        "4   5 2018-05-01       0.5689       3050.18           11.38   \n",
                        "\n",
                        "   costo_insumos_index  confianza_consumidor  \n",
                        "0                99.37                  4.26  \n",
                        "1               101.19                  3.73  \n",
                        "2               101.68                  7.30  \n",
                        "3               102.42                  9.29  \n",
                        "4               104.06                  6.98  \n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n--- SANITY CHECK ---\\n\")\n",
                "\n",
                "# Global dictionary to hold all analysis results per table\n",
                "TABLE_ANALYSIS = {}\n",
                "source_tables = config['data']['source_tables']\n",
                "\n",
                "for table in source_tables:\n",
                "    file_path = RAW_DATA_PATH / f\"{table}.parquet\"\n",
                "    if file_path.exists():\n",
                "        try:\n",
                "            df = pd.read_parquet(file_path)\n",
                "            print(f\"\\n--- {table} ---\")\n",
                "            print(df.info())\n",
                "            print(df.head())\n",
                "            \n",
                "            # Basic Validation checks\n",
                "            val_info = {\n",
                "                \"columns\": list(df.columns),\n",
                "                \"rows\": len(df),\n",
                "            }\n",
                "            \n",
                "            # Critical Check: Ventas History > 36 Months\n",
                "            if table == \"ventas_diarias\":\n",
                "                if date_column in df.columns:\n",
                "                    min_date = df[date_column].min()\n",
                "                    max_date = df[date_column].max()\n",
                "                    if pd.notnull(min_date) and pd.notnull(max_date):\n",
                "                        months_diff = (max_date - min_date) / pd.Timedelta(days=30)\n",
                "                        val_info[\"history_months\"] = round(months_diff, 2)\n",
                "                        \n",
                "                        if months_diff < 36:\n",
                "                            print(f\"CRITICAL WARNING: {table} has less than 36 months history ({months_diff:.1f})\")\n",
                "                            val_info[\"history_check\"] = \"FAIL\"\n",
                "                        else:\n",
                "                            print(f\"SUCCESS: {table} history sufficient ({months_diff:.1f} months)\")\n",
                "                            val_info[\"history_check\"] = \"PASS\"\n",
                "                    else:\n",
                "                         val_info[\"history_check\"] = \"FAIL - Dates Null\"\n",
                "                else:\n",
                "                     val_info[\"history_check\"] = \"FAIL - No Date Col\"\n",
                "            \n",
                "            # Initialize analysis dict\n",
                "            TABLE_ANALYSIS[table] = {\n",
                "                \"validation\": val_info,\n",
                "                \"statistics\": {},\n",
                "                \"temporal\": {},\n",
                "                \"categorical\": {},\n",
                "                \"outliers\": {},\n",
                "                \"zero_variance\": [],\n",
                "                \"high_cardinality\": {},\n",
                "                \"zero_presence\": {},\n",
                "                \"duplicates\": {},\n",
                "                \"null_values\": {},\n",
                "                \"sentinels\": {}\n",
                "            }\n",
                "        except Exception as e:\n",
                "            print(f\"Error reading local file {file_path}: {e}\")\n",
                "    else:\n",
                "        print(f\"WARNING: File for {table} not found at {file_path}.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- STATISTICAL ANALYSIS (NUMERIC) ---\n",
                        "\n",
                        "ventas_diarias: Analyzed 10 numeric columns.\n",
                        "redes_sociales: Analyzed 4 numeric columns.\n",
                        "promocion_diaria: Analyzed 2 numeric columns.\n",
                        "macro_economia: Analyzed 6 numeric columns.\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n--- STATISTICAL ANALYSIS (NUMERIC) ---\\n\")\n",
                "\n",
                "for table in source_tables:\n",
                "    file_path = RAW_DATA_PATH / f\"{table}.parquet\"\n",
                "    if file_path.exists():\n",
                "        try:\n",
                "            df = pd.read_parquet(file_path)\n",
                "            \n",
                "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
                "            stats_dict = {}\n",
                "            \n",
                "            for col in numeric_cols:\n",
                "                desc = df[col].describe()\n",
                "                stats_dict[col] = {\n",
                "                    \"mean\": float(desc['mean']),\n",
                "                    \"median\": float(df[col].median()),\n",
                "                    \"std\": float(desc['std']),\n",
                "                    \"min\": float(desc['min']),\n",
                "                    \"max\": float(desc['max']),\n",
                "                    \"q25\": float(desc['25%']),\n",
                "                    \"q50\": float(desc['50%']),\n",
                "                    \"q75\": float(desc['75%'])\n",
                "                }\n",
                "                \n",
                "            if table in TABLE_ANALYSIS:\n",
                "                TABLE_ANALYSIS[table][\"statistics\"] = stats_dict\n",
                "            \n",
                "            print(f\"{table}: Analyzed {len(numeric_cols)} numeric columns.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Skipping {table}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- TEMPORAL ANALYSIS (DATETIME) ---\n",
                        "\n",
                        "ventas_diarias: Analyzed 1 datetime columns.\n",
                        "redes_sociales: Analyzed 1 datetime columns.\n",
                        "promocion_diaria: Analyzed 1 datetime columns.\n",
                        "macro_economia: Analyzed 1 datetime columns.\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n--- TEMPORAL ANALYSIS (DATETIME) ---\\n\")\n",
                "\n",
                "for table in source_tables:\n",
                "    file_path = RAW_DATA_PATH / f\"{table}.parquet\"\n",
                "    if file_path.exists():\n",
                "        try:\n",
                "            df = pd.read_parquet(file_path)\n",
                "            \n",
                "            if date_column in df.columns:\n",
                "                 df[date_column] = pd.to_datetime(df[date_column])\n",
                "\n",
                "            datetime_cols = df.select_dtypes(include=['datetime64[ns]', 'datetime64', 'datetime']).columns\n",
                "            \n",
                "            temp_dict = {}\n",
                "            \n",
                "            for col in datetime_cols:\n",
                "                series = df[col].dropna()\n",
                "                if series.empty:\n",
                "                    continue\n",
                "                    \n",
                "                min_date = series.min()\n",
                "                max_date = series.max()\n",
                "                \n",
                "                # Check for duplicates\n",
                "                duplicates = series.duplicated().sum()\n",
                "                \n",
                "                # Check for gaps (only if it looks like a daily series)\n",
                "                gaps_info = \"N/A\"\n",
                "                if col == date_column:\n",
                "                    try:\n",
                "                        full_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
                "                        missing_dates = full_range.difference(series)\n",
                "                        gaps_info = len(missing_dates)\n",
                "                    except:\n",
                "                        pass\n",
                "\n",
                "                temp_dict[col] = {\n",
                "                    \"min_date\": str(min_date),\n",
                "                    \"max_date\": str(max_date),\n",
                "                    \"duplicates\": int(duplicates),\n",
                "                    \"missing_days_in_sequence\": gaps_info\n",
                "                }\n",
                "                \n",
                "            if table in TABLE_ANALYSIS:\n",
                "                TABLE_ANALYSIS[table][\"temporal\"] = temp_dict\n",
                "                \n",
                "            print(f\"{table}: Analyzed {len(datetime_cols)} datetime columns.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Skipping {table}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- CATEGORICAL ANALYSIS (OBJECT) ---\n",
                        "\n",
                        "ventas_diarias: Analyzed 0 categorical columns.\n",
                        "redes_sociales: Analyzed 1 categorical columns.\n",
                        "promocion_diaria: Analyzed 0 categorical columns.\n",
                        "macro_economia: Analyzed 0 categorical columns.\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n--- CATEGORICAL ANALYSIS (OBJECT) ---\\n\")\n",
                "\n",
                "for table in source_tables:\n",
                "    file_path = RAW_DATA_PATH / f\"{table}.parquet\"\n",
                "    if file_path.exists():\n",
                "        try:\n",
                "            df = pd.read_parquet(file_path)\n",
                "            \n",
                "            obj_cols = df.select_dtypes(include=['object', 'category']).columns\n",
                "            cat_dict = {}\n",
                "            \n",
                "            for col in obj_cols:\n",
                "                # Frequency and Weight\n",
                "                counts = df[col].value_counts()\n",
                "                total = len(df)\n",
                "                \n",
                "                # Top 10 categories to avoid exploding JSON\n",
                "                top_counts = counts.head(10)\n",
                "                \n",
                "                cat_details = {}\n",
                "                for cat_name, count in top_counts.items():\n",
                "                    weight = (count / total) * 100\n",
                "                    cat_details[str(cat_name)] = {\n",
                "                        \"freq\": int(count),\n",
                "                        \"weight_percent\": round(float(weight), 2)\n",
                "                    }\n",
                "                    \n",
                "                cat_dict[col] = {\n",
                "                    \"unique_count\": int(df[col].nunique()),\n",
                "                    \"top_categories\": cat_details\n",
                "                }\n",
                "                \n",
                "            if table in TABLE_ANALYSIS:\n",
                "                TABLE_ANALYSIS[table][\"categorical\"] = cat_dict\n",
                "                \n",
                "            print(f\"{table}: Analyzed {len(obj_cols)} categorical columns.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Skipping {table}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- OUTLIER DETECTION ---\n",
                        "\n",
                        "ventas_diarias: Outlier analysis complete.\n",
                        "redes_sociales: Outlier analysis complete.\n",
                        "promocion_diaria: Outlier analysis complete.\n",
                        "macro_economia: Outlier analysis complete.\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n--- OUTLIER DETECTION ---\\n\")\n",
                "\n",
                "for table in source_tables:\n",
                "    file_path = RAW_DATA_PATH / f\"{table}.parquet\"\n",
                "    if file_path.exists():\n",
                "        try:\n",
                "            df = pd.read_parquet(file_path)\n",
                "            \n",
                "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
                "            outlier_dict = {}\n",
                "            \n",
                "            for col in numeric_cols:\n",
                "                try:\n",
                "                    q1 = df[col].quantile(0.25)\n",
                "                    q3 = df[col].quantile(0.75)\n",
                "                    iqr = q3 - q1\n",
                "                    \n",
                "                    lower_bound = q1 - 1.5 * iqr\n",
                "                    upper_bound = q3 + 1.5 * iqr\n",
                "                    \n",
                "                    outliers_low = df[df[col] < lower_bound]\n",
                "                    outliers_high = df[df[col] > upper_bound]\n",
                "                    \n",
                "                    outlier_dict[col] = {\n",
                "                        \"lower_limit\": float(lower_bound),\n",
                "                        \"upper_limit\": float(upper_bound),\n",
                "                        \"count_low\": int(len(outliers_low)),\n",
                "                        \"count_high\": int(len(outliers_high)),\n",
                "                        \"total_outliers\": int(len(outliers_low) + len(outliers_high))\n",
                "                    }\n",
                "                except Exception as e:\n",
                "                     print(f\"Skipping outlier check for {col}: {e}\")\n",
                "                \n",
                "            if table in TABLE_ANALYSIS:\n",
                "                TABLE_ANALYSIS[table][\"outliers\"] = outlier_dict\n",
                "                \n",
                "            print(f\"{table}: Outlier analysis complete.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Skipping {table}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- CHECK: ZERO VARIANCE ---\n",
                        "\n",
                        "ventas_diarias: No zero variance columns found.\n",
                        "redes_sociales: No zero variance columns found.\n",
                        "promocion_diaria: No zero variance columns found.\n",
                        "macro_economia: No zero variance columns found.\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n--- CHECK: ZERO VARIANCE ---\\n\")\n",
                "\n",
                "for table in source_tables:\n",
                "    file_path = RAW_DATA_PATH / f\"{table}.parquet\"\n",
                "    if file_path.exists():\n",
                "        try:\n",
                "            df = pd.read_parquet(file_path)\n",
                "            zero_variance_cols = []\n",
                "            for col in df.columns:\n",
                "                if df[col].nunique() <= 1:\n",
                "                    zero_variance_cols.append(col)\n",
                "            \n",
                "            if table in TABLE_ANALYSIS:\n",
                "                TABLE_ANALYSIS[table][\"zero_variance\"] = zero_variance_cols\n",
                "            \n",
                "            if zero_variance_cols:\n",
                "                print(f\"{table}: Found {len(zero_variance_cols)} columns with zero variance: {zero_variance_cols}\")\n",
                "            else:\n",
                "                print(f\"{table}: No zero variance columns found.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Skipping {table}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- CHECK: HIGH CARDINALITY ---\n",
                        "\n",
                        "Using High Cardinality Threshold: 0.9\n",
                        "ventas_diarias: Found 2 columns with high cardinality.\n",
                        "redes_sociales: Found 2 columns with high cardinality.\n",
                        "promocion_diaria: Found 2 columns with high cardinality.\n",
                        "macro_economia: Found 6 columns with high cardinality.\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n--- CHECK: HIGH CARDINALITY ---\\n\")\n",
                "\n",
                "high_card_threshold = config.get('quality', {}).get('high_cardinality_threshold', 50)\n",
                "print(f\"Using High Cardinality Threshold: {high_card_threshold}\")\n",
                "\n",
                "for table in source_tables:\n",
                "    file_path = RAW_DATA_PATH / f\"{table}.parquet\"\n",
                "    if file_path.exists():\n",
                "        try:\n",
                "            df = pd.read_parquet(file_path)\n",
                "            high_card_cols = {}\n",
                "            \n",
                "            # Start with all columns, but typically relevant for categorical\n",
                "            for col in df.columns:\n",
                "                unique_count = df[col].nunique()\n",
                "                \n",
                "                # If threshold is float < 1.0, treat as ratio\n",
                "                if isinstance(high_card_threshold, float) and high_card_threshold < 1.0:\n",
                "                    ratio = unique_count / len(df) if len(df) > 0 else 0\n",
                "                    if ratio > high_card_threshold:\n",
                "                        high_card_cols[col] = {\"unique\": unique_count, \"ratio\": round(ratio, 4)}\n",
                "                else:\n",
                "                    # Treat as absolute count\n",
                "                    if unique_count > high_card_threshold:\n",
                "                         # Optional: Ignore if it looks like a float/continuous variable unless requested\n",
                "                         high_card_cols[col] = {\"unique\": unique_count}\n",
                "            \n",
                "            if table in TABLE_ANALYSIS:\n",
                "                TABLE_ANALYSIS[table][\"high_cardinality\"] = high_card_cols\n",
                "            \n",
                "            if high_card_cols:\n",
                "                print(f\"{table}: Found {len(high_card_cols)} columns with high cardinality.\")\n",
                "            else:\n",
                "                print(f\"{table}: No high cardinality columns found.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Skipping {table}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- CHECK: ZERO PRESENCE ---\n",
                        "\n",
                        "Using Zero Presence Threshold: 0.3\n",
                        "ventas_diarias: Found 2 columns with high zero presence.\n",
                        "redes_sociales: Found 3 columns with high zero presence.\n",
                        "promocion_diaria: Found 1 columns with high zero presence.\n",
                        "macro_economia: No columns with high zero presence found.\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n--- CHECK: ZERO PRESENCE ---\\n\")\n",
                "\n",
                "zero_presence_threshold = config.get('quality', {}).get('zero_presence_threshold', 0.5)\n",
                "print(f\"Using Zero Presence Threshold: {zero_presence_threshold}\")\n",
                "\n",
                "for table in source_tables:\n",
                "    file_path = RAW_DATA_PATH / f\"{table}.parquet\"\n",
                "    if file_path.exists():\n",
                "        try:\n",
                "            df = pd.read_parquet(file_path)\n",
                "            zero_presence_cols = {}\n",
                "            \n",
                "            # Check only numeric columns usually\n",
                "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
                "            \n",
                "            for col in numeric_cols:\n",
                "                zero_count = (df[col] == 0).sum()\n",
                "                total_count = len(df)\n",
                "                ratio = zero_count / total_count if total_count > 0 else 0\n",
                "                \n",
                "                if ratio > zero_presence_threshold:\n",
                "                    zero_presence_cols[col] = {\n",
                "                        \"zero_count\": int(zero_count),\n",
                "                        \"ratio\": round(ratio, 4)\n",
                "                    }\n",
                "            \n",
                "            if table in TABLE_ANALYSIS:\n",
                "                TABLE_ANALYSIS[table][\"zero_presence\"] = zero_presence_cols\n",
                "            \n",
                "            if zero_presence_cols:\n",
                "                 print(f\"{table}: Found {len(zero_presence_cols)} columns with high zero presence.\")\n",
                "            else:\n",
                "                 print(f\"{table}: No columns with high zero presence found.\")\n",
                "\n",
                "        except Exception as e:\n",
                "            print(f\"Skipping {table}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- CHECK: REPEATED ROWS ---\n",
                        "\n",
                        "ventas_diarias: No duplicated rows found.\n",
                        "redes_sociales: No duplicated rows found.\n",
                        "promocion_diaria: No duplicated rows found.\n",
                        "macro_economia: No duplicated rows found.\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n--- CHECK: REPEATED ROWS ---\\n\")\n",
                "\n",
                "for table in source_tables:\n",
                "    file_path = RAW_DATA_PATH / f\"{table}.parquet\"\n",
                "    if file_path.exists():\n",
                "        try:\n",
                "            df = pd.read_parquet(file_path)\n",
                "            \n",
                "            duplicates_count = df.duplicated().sum()\n",
                "            \n",
                "            if table in TABLE_ANALYSIS:\n",
                "                TABLE_ANALYSIS[table][\"duplicates\"] = {\n",
                "                    \"count\": int(duplicates_count),\n",
                "                    \"has_duplicates\": bool(duplicates_count > 0)\n",
                "                }\n",
                "            \n",
                "            if duplicates_count > 0:\n",
                "                print(f\"{table}: Found {duplicates_count} duplicated rows.\")\n",
                "            else:\n",
                "                print(f\"{table}: No duplicated rows found.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Skipping {table}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- CHECK: NULL VALUES ---\n",
                        "\n",
                        "ventas_diarias: No null values found.\n",
                        "redes_sociales: No null values found.\n",
                        "promocion_diaria: No null values found.\n",
                        "macro_economia: No null values found.\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n--- CHECK: NULL VALUES ---\\n\")\n",
                "\n",
                "for table in source_tables:\n",
                "    file_path = RAW_DATA_PATH / f\"{table}.parquet\"\n",
                "    if file_path.exists():\n",
                "        try:\n",
                "            df = pd.read_parquet(file_path)\n",
                "            null_cols = {}\n",
                "            \n",
                "            for col in df.columns:\n",
                "                null_count = df[col].isnull().sum()\n",
                "                if null_count > 0:\n",
                "                    ratio = null_count / len(df)\n",
                "                    null_cols[col] = {\n",
                "                        \"count\": int(null_count),\n",
                "                        \"ratio\": round(ratio, 4)\n",
                "                    }\n",
                "            \n",
                "            if table in TABLE_ANALYSIS:\n",
                "                TABLE_ANALYSIS[table][\"null_values\"] = null_cols\n",
                "            \n",
                "            if null_cols:\n",
                "                print(f\"{table}: Found {len(null_cols)} columns with null values.\")\n",
                "            else:\n",
                "                print(f\"{table}: No null values found.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Skipping {table}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- CHECK: SENTINEL VALUES ---\n",
                        "\n",
                        "ventas_diarias: Found sentinel values in 1 columns.\n",
                        "redes_sociales: Found sentinel values in 1 columns.\n",
                        "promocion_diaria: Found sentinel values in 1 columns.\n",
                        "macro_economia: Found sentinel values in 1 columns.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\USUARIO\\AppData\\Local\\Temp\\ipykernel_13444\\3889766503.py:19: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
                        "  elif pd.api.types.is_object_dtype(dtype) or pd.api.types.is_categorical_dtype(dtype):\n",
                        "C:\\Users\\USUARIO\\AppData\\Local\\Temp\\ipykernel_13444\\3889766503.py:19: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
                        "  elif pd.api.types.is_object_dtype(dtype) or pd.api.types.is_categorical_dtype(dtype):\n",
                        "C:\\Users\\USUARIO\\AppData\\Local\\Temp\\ipykernel_13444\\3889766503.py:19: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
                        "  elif pd.api.types.is_object_dtype(dtype) or pd.api.types.is_categorical_dtype(dtype):\n",
                        "C:\\Users\\USUARIO\\AppData\\Local\\Temp\\ipykernel_13444\\3889766503.py:19: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
                        "  elif pd.api.types.is_object_dtype(dtype) or pd.api.types.is_categorical_dtype(dtype):\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n--- CHECK: SENTINEL VALUES ---\\n\")\n",
                "\n",
                "sentinel_config = config.get('quality', {}).get('sentinel_values', {})\n",
                "\n",
                "for table in source_tables:\n",
                "    file_path = RAW_DATA_PATH / f\"{table}.parquet\"\n",
                "    if file_path.exists():\n",
                "        try:\n",
                "            df = pd.read_parquet(file_path)\n",
                "            sentinel_report = {}\n",
                "            \n",
                "            for col in df.columns:\n",
                "                dtype = df[col].dtype\n",
                "                sentinels_to_check = []\n",
                "                \n",
                "                # Determine which sentinels to check based on dtype\n",
                "                if pd.api.types.is_numeric_dtype(dtype):\n",
                "                    sentinels_to_check = sentinel_config.get('numeric', [])\n",
                "                elif pd.api.types.is_object_dtype(dtype) or pd.api.types.is_categorical_dtype(dtype):\n",
                "                    sentinels_to_check = sentinel_config.get('categorical', [])\n",
                "                elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
                "                    sentinels_to_check = sentinel_config.get('datetime', [])\n",
                "                elif pd.api.types.is_bool_dtype(dtype):\n",
                "                    sentinels_to_check = sentinel_config.get('boolean', [])\n",
                "                \n",
                "                # Check column against its relevant sentinels\n",
                "                for sentinel in sentinels_to_check:\n",
                "                    # Use isin for robustness or simple equality\n",
                "                    try:\n",
                "                        if pd.api.types.is_datetime64_any_dtype(dtype):\n",
                "                             # Handle date comparison carefully\n",
                "                             count = (df[col] == pd.Timestamp(sentinel)).sum()\n",
                "                        else:\n",
                "                             count = (df[col] == sentinel).sum()\n",
                "                        \n",
                "                        if count > 0:\n",
                "                            if col not in sentinel_report:\n",
                "                                sentinel_report[col] = []\n",
                "                            sentinel_report[col].append({\n",
                "                                \"value\": sentinel,\n",
                "                                \"count\": int(count)\n",
                "                            })\n",
                "                    except Exception:\n",
                "                        # Ignore comparison errors (e.g. string vs int)\n",
                "                        pass\n",
                "            \n",
                "            if table in TABLE_ANALYSIS:\n",
                "                TABLE_ANALYSIS[table][\"sentinels\"] = sentinel_report\n",
                "            \n",
                "            if sentinel_report:\n",
                "                print(f\"{table}: Found sentinel values in {len(sentinel_report)} columns.\")\n",
                "            else:\n",
                "                print(f\"{table}: No sentinel values found.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Skipping {table}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Full Report saved to experiments\\phase_01_discovery\\artifacts\\phase_01_discovery.json\n"
                    ]
                }
            ],
            "source": [
                "# Consolidate Report\n",
                "final_report = {\n",
                "    \"phase\": \"Phase 1 - Data Discovery\",\n",
                "    \"timestamp\": datetime.datetime.now().isoformat(),\n",
                "    \"description\": \"Incremental download and detailed statistical analysis (Includes Variance, Cardinality, Zeros, Duplicates, Nulls, Sentinels).\",\n",
                "    \"download_details\": download_details,\n",
                "    \"data_analysis\": TABLE_ANALYSIS\n",
                "}\n",
                "\n",
                "report_path = EXPERIMENT_ARTIFACTS_PATH / \"phase_01_discovery.json\"\n",
                "with open(report_path, \"w\") as f:\n",
                "    json.dump(final_report, f, indent=4)\n",
                "\n",
                "print(f\"\\nFull Report saved to {report_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
